{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import auc, roc_curve, confusion_matrix\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "import torch\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from scfoundation import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, tokenizer_dir, adata, pad_value, pad_token):\n",
    "        self.tokenizer_dir = tokenizer_dir\n",
    "        self.adata = adata\n",
    "        self.pad_value = pad_value\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def prepare_data(self):\n",
    "        gexpr_feature = self.adata.X.A\n",
    "        S = gexpr_feature.sum(1)\n",
    "        T = S\n",
    "        TS = np.concatenate([[np.log10(T)],[np.log10(S)]],axis=0).T\n",
    "        data = np.concatenate([gexpr_feature,TS],axis=1)\n",
    "        self.data = data\n",
    "\n",
    "        self.celltype_labels = self.adata.obs['cell_type'].cat.codes.values\n",
    "\n",
    "    def prepare_train_and_valid_data(self, train_index, valid_index):\n",
    "        celltype_labels_train = torch.from_numpy(self.celltype_labels[train_index]).long()\n",
    "        celltype_labels_valid = torch.from_numpy(self.celltype_labels[valid_index]).long()\n",
    "\n",
    "        train_data = torch.from_numpy(self.data[train_index]).float()\n",
    "        valid_data = torch.from_numpy(self.data[valid_index]).float()\n",
    "        train_data_gene_ids = torch.arange(train_data.shape[1]).repeat(train_data.shape[0], 1)\n",
    "        valid_data_gene_ids = torch.arange(valid_data.shape[1]).repeat(valid_data.shape[0], 1)\n",
    "\n",
    "        train_data_index = train_data != 0\n",
    "        train_values, train_padding = load.gatherData(train_data, train_data_index, self.pad_value)\n",
    "        train_gene_ids, _ = load.gatherData(train_data_gene_ids, train_data_index, self.pad_token)\n",
    "        train_data = {'values': train_values, 'padding': train_padding, 'gene_ids': train_gene_ids, 'celltype_labels': celltype_labels_train}\n",
    "\n",
    "        valid_data_index = valid_data != 0\n",
    "        valid_values, valid_padding = load.gatherData(valid_data, valid_data_index, self.pad_value)\n",
    "        valid_gene_ids, _ = load.gatherData(valid_data_gene_ids, valid_data_index, self.pad_token)\n",
    "        valid_data = {'values': valid_values, 'padding': valid_padding, 'gene_ids': valid_gene_ids, 'celltype_labels': celltype_labels_valid}\n",
    "\n",
    "        return train_data, valid_data\n",
    "\n",
    "\n",
    "def downsample(adata, n):\n",
    "    celltype_counts = adata.obs['cell_type'].value_counts() \n",
    "    to_downsample = celltype_counts[celltype_counts > 400].index.tolist()\n",
    "\n",
    "    keep_cells = np.zeros(len(adata), dtype=bool)\n",
    "    for ct in adata.obs['cell_type'].unique():\n",
    "        ct_mask = adata.obs['cell_type'] == ct\n",
    "        if ct in to_downsample:\n",
    "            np.random.seed(0)\n",
    "            sampled_idx = np.random.choice(\n",
    "                np.where(ct_mask)[0],  \n",
    "                size=n,               \n",
    "                replace=False           \n",
    "            )\n",
    "            keep_cells[sampled_idx] = True  \n",
    "\n",
    "    adata = adata[keep_cells]\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scF_Cls(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            scf_token_emb,\n",
    "            scf_pos_emb,\n",
    "            scf_encoder,\n",
    "            d_model: int,\n",
    "            n_cls: int = 2,\n",
    "            nlayers_cls: int = 3,\n",
    "    ):\n",
    "        super(scF_Cls, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.token_emb = scf_token_emb\n",
    "        self.pos_emb = scf_pos_emb\n",
    "        self.encoder = scf_encoder\n",
    "        self.cls_decoder = ClsDecoder(d_model, n_cls, nlayers=nlayers_cls)\n",
    "\n",
    "    def forward(self, gene_values, padding_label, gene_ids):\n",
    "\n",
    "        # token and positional embedding\n",
    "        x = self.token_emb(torch.unsqueeze(gene_values, 2), output_weight = 0)\n",
    "\n",
    "        position_emb = self.pos_emb(gene_ids)\n",
    "        x += position_emb\n",
    "        x = self.encoder(x, padding_mask=padding_label)\n",
    "\n",
    "        geneembmerge = torch.cat([torch.max(x[k][~padding_label[k]], dim=0)[0].unsqueeze(0) for k in range(x.size(0))])\n",
    "        output = self.cls_decoder(geneembmerge)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class ClsDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder for cell classification task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_cls: int,\n",
    "        nlayers: int = 3,\n",
    "        activation: callable = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # module list\n",
    "        self._decoder = nn.ModuleList()\n",
    "        for i in range(nlayers - 1):\n",
    "            self._decoder.append(nn.Linear(d_model, d_model))\n",
    "            self._decoder.append(activation())\n",
    "            self._decoder.append(nn.LayerNorm(d_model))\n",
    "        self.out_layer = nn.Linear(d_model, n_cls)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, embsize]\n",
    "        \"\"\"\n",
    "        for layer in self._decoder:\n",
    "            x = layer(x)\n",
    "        return self.out_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model: nn.Module, train_data, batch_size, scaler, optimizer, scheduler, log_interval, epoch) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    amp = True\n",
    "\n",
    "    model.train()\n",
    "    total_cls = 0.0\n",
    "    total_error = 0.0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_values = train_data['values']\n",
    "    train_padding = train_data['padding']\n",
    "    train_gene_ids = train_data['gene_ids']\n",
    "    train_celltype_labels = train_data['celltype_labels']\n",
    "\n",
    "    num_batches = np.ceil(len(train_values)/batch_size).astype(int)\n",
    "    for k in range(0, len(train_values), batch_size):\n",
    "        batch = int(k/batch_size+1)\n",
    "        if k+12>=len(train_values):\n",
    "            break\n",
    "        with torch.cuda.amp.autocast(enabled=amp):\n",
    "            output = model(train_values[k:k+batch_size].to(device), \n",
    "                           train_padding[k:k+batch_size].to(device), \n",
    "                           train_gene_ids[k:k+batch_size].to(device))\n",
    "            \n",
    "            batch_celltype_labels = train_celltype_labels[k:k+batch_size].to(device)\n",
    "            loss_cls = criterion_cls(output, batch_celltype_labels)\n",
    "\n",
    "            error_rate_cls = 1 - (\n",
    "                    (output.argmax(1) == batch_celltype_labels)\n",
    "                    .sum()\n",
    "                    .item()\n",
    "                ) / batch_celltype_labels.size(0)\n",
    "            \n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss_cls).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_cls += loss_cls.item()\n",
    "        total_error += error_rate_cls\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            sec_per_batch = (time.time() - start_time) / log_interval\n",
    "            cur_cls = total_cls / log_interval\n",
    "            cur_error = total_error / log_interval\n",
    "            print(f\"| Split {split} | {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.8f} | sec/batch {sec_per_batch:5.1f} | \"\n",
    "                f\"cls {cur_cls:5.5f} | \"\n",
    "                f\"acc {1-cur_error:1.5f} | \"\n",
    "            )\n",
    "            total_cls = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, valid_data, batch_size) -> None:\n",
    "    amp = True\n",
    "    \n",
    "    model.eval()\n",
    "    total_cls = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_num = 0\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    valid_values = valid_data['values']\n",
    "    valid_padding = valid_data['padding']\n",
    "    valid_gene_ids = valid_data['gene_ids']\n",
    "    valid_celltype_labels = valid_data['celltype_labels']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for k in tqdm(range(0, len(valid_values), batch_size)):\n",
    "            if k+12>=len(valid_values):\n",
    "                break\n",
    "            with torch.cuda.amp.autocast(enabled=amp):\n",
    "                output = model(valid_values[k:k+batch_size].to(device), \n",
    "                           valid_padding[k:k+batch_size].to(device), \n",
    "                           valid_gene_ids[k:k+batch_size].to(device))\n",
    "            \n",
    "                batch_celltype_labels = valid_celltype_labels[k:k+batch_size].to(device)\n",
    "                loss_cls = criterion_cls(output, batch_celltype_labels)\n",
    "            \n",
    "            accuracy = (output.argmax(1) == batch_celltype_labels).sum().item()\n",
    "            total_accuracy += accuracy\n",
    "            total_num += batch_celltype_labels.size(0)\n",
    "            total_cls += loss_cls*batch_celltype_labels.size(0)\n",
    "\n",
    "            true_labels.append(batch_celltype_labels.to('cpu'))\n",
    "            predicted_labels.append(output.argmax(1).to('cpu'))\n",
    "\n",
    "    val_acc = total_accuracy / total_num\n",
    "    val_loss = total_cls / total_num\n",
    "\n",
    "    true_labels = torch.cat(true_labels).numpy()\n",
    "    predicted_labels = torch.cat(predicted_labels).numpy()\n",
    "\n",
    "    return val_loss, val_acc, true_labels, predicted_labels\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_data, valid_data, batch_size, epochs):\n",
    "    amp = True\n",
    "    lr = 1e-4\n",
    "    schedule_ratio = 0.9\n",
    "    schedule_interval = 1\n",
    "    log_interval = 10\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=lr, eps=1e-4 if amp else 1e-8\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, schedule_interval, gamma=schedule_ratio\n",
    "    )\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "    best_val_accuracy = 0\n",
    "    best_model = None\n",
    "    best_true_labels = None\n",
    "    best_predicted_labels = None\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train(model, train_data, batch_size, scaler, optimizer, scheduler, log_interval, epoch)\n",
    "\n",
    "        val_loss, val_accuracy, true_labels, predicted_labels = evaluate(model, valid_data, batch_size)\n",
    "        print(\"-\" * 89)\n",
    "        print(\n",
    "            f\"| end of epoch {epoch:3d} | \"\n",
    "            f\"valid accuracy: {val_accuracy:1.4f} | \"\n",
    "            f\"valid loss: {val_loss:5.4f} | \"\n",
    "        )\n",
    "        print(\"-\" * 89)\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model = copy.deepcopy(model)\n",
    "            print(f\"Best model with accuracy {best_val_accuracy:1.4f}\")\n",
    "\n",
    "            best_true_labels = true_labels\n",
    "            best_predicted_labels = predicted_labels\n",
    "\n",
    "        scheduler.step()\n",
    "    \n",
    "    return best_val_accuracy, best_model, best_true_labels, best_predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scFoundation(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            scf_token_emb,\n",
    "            scf_pos_emb,\n",
    "            scf_encoder,\n",
    "            scf_decoder,\n",
    "            scf_decoder_embed,\n",
    "            scf_norm,\n",
    "            scf_to_final,\n",
    "    ):\n",
    "        super(scFoundation, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.token_emb = scf_token_emb\n",
    "        self.pos_emb = scf_pos_emb\n",
    "\n",
    "        # ## DEBUG\n",
    "        self.encoder = scf_encoder\n",
    "\n",
    "        ##### decoder\n",
    "        self.decoder = scf_decoder\n",
    "        self.decoder_embed = scf_decoder_embed\n",
    "        self.norm = scf_norm\n",
    "        self.to_final = scf_to_final\n",
    "\n",
    "    def forward(self, x, padding_label, encoder_position_gene_ids, encoder_labels, decoder_data,\n",
    "                decoder_position_gene_ids, decoder_data_padding_labels, **kwargs):\n",
    "\n",
    "        # token and positional embedding\n",
    "        x = self.token_emb(torch.unsqueeze(x, 2), output_weight = 0)\n",
    "\n",
    "        position_emb = self.pos_emb(encoder_position_gene_ids)\n",
    "        x += position_emb\n",
    "        x = self.encoder(x, padding_mask=padding_label)\n",
    "\n",
    "        decoder_data = self.token_emb(torch.unsqueeze(decoder_data, 2))\n",
    "        position_emb = self.pos_emb(decoder_position_gene_ids)\n",
    "        batch_idx, gen_idx = (encoder_labels == True).nonzero(as_tuple=True)\n",
    "        decoder_data[batch_idx, gen_idx] = x[~padding_label].to(decoder_data.dtype)\n",
    "\n",
    "        decoder_data += position_emb\n",
    "\n",
    "        decoder_data = self.decoder_embed(decoder_data)\n",
    "        x = self.decoder(decoder_data, padding_mask=decoder_data_padding_labels)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        # return x\n",
    "        x = self.to_final(x)\n",
    "        return x.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_file, n_cls):\n",
    "    if model_file is None:\n",
    "        # load pretrained model\n",
    "        pretrainmodel, pretrainconfig = load.load_model_frommmf('scfoundation/models/models.ckpt')\n",
    "    else:\n",
    "        # load fine-tuned model\n",
    "        pretrainmodel = torch.load(f'fine-tuning_mse/models/{model_file}', map_location='cpu')\n",
    "        pretrainmodel = pretrainmodel.module\n",
    "    \n",
    "    model = scF_Cls(pretrainmodel.token_emb,\n",
    "            pretrainmodel.pos_emb,\n",
    "            pretrainmodel.encoder,\n",
    "            d_model = 768,\n",
    "            n_cls = n_cls,\n",
    "            nlayers_cls = 3\n",
    "            )\n",
    "    \n",
    "    pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "    for name, para in model.named_parameters():\n",
    "        para.requires_grad = False\n",
    "    for name, para in model.encoder.transformer_encoder[10:12].named_parameters():\n",
    "        para.requires_grad = True        \n",
    "    for name, para in model.cls_decoder.named_parameters():\n",
    "        para.requires_grad = True\n",
    "    post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "    print(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "    print(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = 19266\n",
    "pad_value = 103\n",
    "tokenizer_dir = '../stformer/tokenizer/'\n",
    "\n",
    "model_file = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'pancreas_cosmx'\n",
    "adata = sc.read_h5ad(f'../datasets/pancreas_cosmx_niche.h5ad')\n",
    "\n",
    "adata.obs['cell_type'] = adata.obs['cell_type'].map(dict(zip(np.array(range(len(adata.uns['cell_types_list'])))+1, adata.uns['cell_types_list'])))\n",
    "adata.obs['cell_type'] = adata.obs['cell_type'].astype('category')\n",
    "\n",
    "adata = downsample(adata, 490)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(tokenizer_dir, adata, pad_value, pad_token)\n",
    "tokenizer.prepare_data()\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "best_val_accuracy_list = []\n",
    "best_true_labels_list = []\n",
    "best_predicted_labels_list = []\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "\n",
    "split = 0\n",
    "for train_index, valid_index in skf.split(tokenizer.adata.obs_names.values, tokenizer.celltype_labels):\n",
    "    split += 1\n",
    "    print(f\"Cross-validate on dataset {dataset} - split {split}\")\n",
    "    train_data, valid_data = tokenizer.prepare_train_and_valid_data(train_index, valid_index)\n",
    "\n",
    "    model = initialize_model(model_file, len(set(tokenizer.celltype_labels)))\n",
    "    model = torch.nn.DataParallel(model, device_ids=[1, 3, 0, 2])\n",
    "    device = torch.device(\"cuda:1\")\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_accuracy, best_model, best_true_labels, best_predicted_labels = train_and_evaluate(model, train_data, valid_data, batch_size, epochs)\n",
    "    best_val_accuracy_list.append(best_val_accuracy)\n",
    "    best_true_labels_list.append(best_true_labels)\n",
    "    best_predicted_labels_list.append(best_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean accuracy: {np.median(best_val_accuracy_list)} +/- {np.std(best_val_accuracy_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_val_accuracy_list, open(f'figures/cell_classification/cv_accuracy_list_scf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_list=[]\n",
    "for i in range(n_splits):\n",
    "    cm = confusion_matrix(best_true_labels_list[i], best_predicted_labels_list[i], normalize='true')\n",
    "    cm_list.append(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cm_list, open(f'figures/cell_classification/cm_list_scf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave out one FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'pancreas_cosmx'\n",
    "adata = sc.read_h5ad(f'../datasets/pancreas_cosmx_niche.h5ad')\n",
    "\n",
    "max_niche_cell_num = 20\n",
    "adata.obsm['niche_celltypes'] = adata.obsm[f'niche_celltypes_niche{max_niche_cell_num}']\n",
    "adata.obsm['niche_composition'] = adata.obsm[f'niche_composition_niche{max_niche_cell_num}']\n",
    "adata.obsm['niche_ligands_expression'] = adata.obsm[f'niche_ligands_expression_niche{max_niche_cell_num}']\n",
    "\n",
    "adata.obs['cell_type'] = adata.obs['cell_type'].map(dict(zip(np.array(range(len(adata.uns['cell_types_list'])))+1, adata.uns['cell_types_list'])))\n",
    "adata.obs['cell_type'] = adata.obs['cell_type'].astype('category')\n",
    "\n",
    "adata1 = adata[adata.obs['fov']!=52]\n",
    "adata1 = downsample(adata1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = Tokenizer(tokenizer_dir, adata, pad_value, pad_token)\n",
    "tokenizer.prepare_data()\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "split = 0\n",
    "train_index, valid_index = train_test_split(np.array(range(tokenizer.adata.shape[0])), test_size=0.2, random_state=42, stratify=tokenizer.celltype_labels)\n",
    "\n",
    "train_data, valid_data = tokenizer.prepare_train_and_valid_data(train_index, valid_index)\n",
    "\n",
    "model = initialize_model(model_file, len(set(tokenizer.celltype_labels)))\n",
    "model = torch.nn.DataParallel(model, device_ids=[1, 3, 0, 2])\n",
    "device = torch.device(\"cuda:1\")\n",
    "model.to(device)\n",
    "\n",
    "best_val_accuracy, best_model, best_true_labels, best_predicted_labels = train_and_evaluate(model, train_data, valid_data, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata2 = adata[adata.obs['fov']==52]\n",
    "adata3 = adata2[adata2.obs['cell_type'].isin(adata1.obs['cell_type'].cat.categories)]\n",
    "\n",
    "tokenizer = Tokenizer(tokenizer_dir, adata3, pad_value, pad_token)\n",
    "tokenizer.prepare_data()\n",
    "\n",
    "split = 0\n",
    "test_index = np.array(range(adata3.shape[0]))\n",
    "test_data, test_data = tokenizer.prepare_train_and_valid_data(test_index, test_index)\n",
    "\n",
    "test_loss, test_accuracy, true_labels, predicted_labels = evaluate(best_model, test_data, batch_size)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true_labels, predicted_labels, normalize='true')\n",
    "pickle.dump(cm, open(f'figures/cell_classification/cm_fov52_scf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata2.obs['predicted_celltypes'] = ['']*adata2.shape[0]\n",
    "predicted_celltypes = [adata3.obs['cell_type'].cat.categories[i] for i in predicted_labels]\n",
    "adata2.obs['predicted_celltypes'].loc[adata3.obs_names] = predicted_celltypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata2.write('figures/cell_classification/adata_fov52_scf.h5ad')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stformer",
   "language": "python",
   "name": "stformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
