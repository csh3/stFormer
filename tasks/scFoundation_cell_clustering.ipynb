{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "import torch\n",
    "from torch import nn\n",
    "from scfoundation import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, adata, pad_value, pad_token):\n",
    "        self.adata = adata\n",
    "        self.pad_value = pad_value\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def prepare_data(self):\n",
    "        gexpr_feature = self.adata.X.A\n",
    "        S = gexpr_feature.sum(1)\n",
    "        T = S\n",
    "        TS = np.concatenate([[np.log10(T)],[np.log10(S)]],axis=0).T\n",
    "        data = np.concatenate([gexpr_feature,TS],axis=1)\n",
    "        self.data = data\n",
    "\n",
    "    def tokenize_data(self):\n",
    "        data = torch.from_numpy(self.data).float()\n",
    "        data_gene_ids = torch.arange(data.shape[1]).repeat(data.shape[0], 1)\n",
    "\n",
    "        data_index = data != 0\n",
    "        gene_values, gene_padding = load.gatherData(data, data_index, self.pad_value)\n",
    "        gene_ids, _ = load.gatherData(data_gene_ids, data_index, self.pad_token)\n",
    "        data = {'values': gene_values, 'padding': gene_padding, 'gene_ids': gene_ids}\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scF_Ccl(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            scf_token_emb,\n",
    "            scf_pos_emb,\n",
    "            scf_encoder,\n",
    "    ):\n",
    "        super(scF_Ccl, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.token_emb = scf_token_emb\n",
    "        self.pos_emb = scf_pos_emb\n",
    "        self.encoder = scf_encoder\n",
    "\n",
    "    def forward(self, gene_values, padding_label, gene_ids):\n",
    "\n",
    "        x = self.token_emb(torch.unsqueeze(gene_values, 2), output_weight = 0)\n",
    "\n",
    "        position_emb = self.pos_emb(gene_ids)\n",
    "        x += position_emb\n",
    "        x = self.encoder(x, padding_mask=padding_label)\n",
    "        geneembmerge = torch.cat([torch.max(x[k][~padding_label[k]], dim=0)[0].unsqueeze(0) for k in range(x.size(0))])\n",
    "        # geneembmerge, _ = torch.max(x, dim=1)\n",
    "\n",
    "        return geneembmerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, data, batch_size) -> None:\n",
    "    model.eval()\n",
    "\n",
    "    gene_values = data['values']\n",
    "    gene_padding = data['padding']\n",
    "    gene_ids = data['gene_ids']\n",
    "\n",
    "    cell_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for k in tqdm(range(0, len(gene_values), batch_size)):\n",
    "            with torch.cuda.amp.autocast(enabled=amp):\n",
    "                geneembmerge = model(gene_values[k:k+batch_size].to(device), \n",
    "                               gene_padding[k:k+batch_size].to(device), \n",
    "                               gene_ids[k:k+batch_size].to(device))\n",
    "                \n",
    "                cell_embeddings.append(geneembmerge.to('cpu'))\n",
    "    \n",
    "    return(torch.cat(cell_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scFoundation(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            scf_token_emb,\n",
    "            scf_pos_emb,\n",
    "            scf_encoder,\n",
    "            scf_decoder,\n",
    "            scf_decoder_embed,\n",
    "            scf_norm,\n",
    "            scf_to_final,\n",
    "    ):\n",
    "        super(scFoundation, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.token_emb = scf_token_emb\n",
    "        self.pos_emb = scf_pos_emb\n",
    "\n",
    "        # ## DEBUG\n",
    "        self.encoder = scf_encoder\n",
    "\n",
    "        ##### decoder\n",
    "        self.decoder = scf_decoder\n",
    "        self.decoder_embed = scf_decoder_embed\n",
    "        self.norm = scf_norm\n",
    "        self.to_final = scf_to_final\n",
    "\n",
    "    def forward(self, x, padding_label, encoder_position_gene_ids, encoder_labels, decoder_data,\n",
    "                decoder_position_gene_ids, decoder_data_padding_labels, **kwargs):\n",
    "\n",
    "        # token and positional embedding\n",
    "        x = self.token_emb(torch.unsqueeze(x, 2), output_weight = 0)\n",
    "\n",
    "        position_emb = self.pos_emb(encoder_position_gene_ids)\n",
    "        x += position_emb\n",
    "        x = self.encoder(x, padding_mask=padding_label)\n",
    "\n",
    "        decoder_data = self.token_emb(torch.unsqueeze(decoder_data, 2))\n",
    "        position_emb = self.pos_emb(decoder_position_gene_ids)\n",
    "        batch_idx, gen_idx = (encoder_labels == True).nonzero(as_tuple=True)\n",
    "        decoder_data[batch_idx, gen_idx] = x[~padding_label].to(decoder_data.dtype)\n",
    "\n",
    "        decoder_data += position_emb\n",
    "\n",
    "        decoder_data = self.decoder_embed(decoder_data)\n",
    "        x = self.decoder(decoder_data, padding_mask=decoder_data_padding_labels)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        # return x\n",
    "        x = self.to_final(x)\n",
    "        return x.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'pretrained' #'fine-tuned'\n",
    "#model_file = 'ft-scf-10X001.ckpt'\n",
    "\n",
    "if model_type == 'pretrained':\n",
    "    pretrainmodel, pretrainconfig = load.load_model_frommmf('scfoundation/models/models.ckpt')\n",
    "elif model_type == 'fine-tuned':\n",
    "    pretrainmodel = torch.load(f'fine-tuning_mse/models/{model_file}', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = scF_Ccl(pretrainmodel.token_emb,\n",
    "            pretrainmodel.pos_emb,\n",
    "            pretrainmodel.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\")\n",
    "\n",
    "model = nn.DataParallel(model, device_ids = [2, 3, 0, 1])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = 19266\n",
    "pad_value = 103\n",
    "\n",
    "tokenizer_dir = '../tokenizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide = '10X001'\n",
    "adata = sc.read_h5ad(f'../datasets/{slide}_niche.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "amp = True\n",
    "\n",
    "tokenizer = Tokenizer(adata, pad_value, pad_token)\n",
    "tokenizer.prepare_data()\n",
    "data = tokenizer.tokenize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_embeddings = evaluate(model,data, batch_size)\n",
    "tokenizer.adata.obsm['cell_embeddings'] = cell_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.adata.obs['cell_type'] = tokenizer.adata.obs['cell_type'].map(dict(zip(np.array(range(len(tokenizer.adata.uns['cell_types_list'])))+1, tokenizer.adata.uns['cell_types_list'])))\n",
    "tokenizer.adata.obs['cell_type'] = tokenizer.adata.obs['cell_type'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(tokenizer.adata, use_rep='cell_embeddings')\n",
    "sc.tl.umap(tokenizer.adata)\n",
    "sc.pl.umap(tokenizer.adata,\n",
    "         #   title='',\n",
    "         #   frameon=False,\n",
    "         #   legend_loc='',\n",
    "         #   legend_fontsize='xx-small',\n",
    "           color='cell_type')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "plt.savefig(f'figures/cell_clustering/umap_scf_pt_{slide}.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scib\n",
    "\n",
    "tokenizer.adata.obs['batch'] = [1]*tokenizer.adata.shape[0]\n",
    "\n",
    "results = scib.metrics.metrics(\n",
    "        tokenizer.adata,\n",
    "        adata_int=tokenizer.adata,\n",
    "        label_key='cell_type',\n",
    "        batch_key='batch',\n",
    "        embed='cell_embeddings',\n",
    "        silhouette_=True,\n",
    "        nmi_=True,\n",
    "        ari_=True,\n",
    "    )\n",
    "\n",
    "result_dict = results[0].to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stformer",
   "language": "python",
   "name": "stformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
