{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from scfoundation import load\n",
    "from stformer.tokenizer import GeneVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, tokenizer_dir, adata, label_path, vocab, pad_value, pad_token, pathway_ligands=None):\n",
    "        self.tokenizer_dir = tokenizer_dir\n",
    "        self.adata = adata\n",
    "        self.label_path = label_path\n",
    "        self.vocab = vocab\n",
    "        self.pad_value = pad_value\n",
    "        self.pad_token = pad_token\n",
    "        self.pathway_ligands = pathway_ligands\n",
    "\n",
    "    def prepare_data(self):\n",
    "        gene_list_df = pd.read_csv(f'{self.tokenizer_dir}/OS_scRNA_gene_index.19264.tsv', header=0, delimiter='\\t')\n",
    "        gene_list = list(gene_list_df['gene_name'])\n",
    "        self.gene_ids = np.array(self.vocab(gene_list), dtype=int)\n",
    "\n",
    "        ligand_database = pd.read_csv(self.tokenizer_dir+'ligand_database.csv', header=0, index_col=0)\n",
    "        ligand_symbol = ligand_database[ligand_database.sum(1)>1].index.values\n",
    "        ligand_symbol = gene_list_df.loc[gene_list_df['gene_name'].isin(ligand_symbol), 'gene_name'].values\n",
    "        self.ligand_ids = np.array(self.vocab(ligand_symbol.tolist())*25, dtype=int)\n",
    "\n",
    "        df_label = pd.read_csv(f'{self.label_path}', header=0)\n",
    "        selected_gene_index = sc.pp.filter_genes(self.adata, min_cells=100, inplace=False)\n",
    "        selected_gene_names = self.adata.var_names.values[selected_gene_index[0]]\n",
    "        df_label = df_label[df_label['gene_symbols'].isin(selected_gene_names)]\n",
    "\n",
    "        selected_cell_index = []\n",
    "        if self.pathway_ligands is not None:\n",
    "            pathway_ligands = list(set(self.pathway_ligands).intersection(set(df_label.loc[df_label['class']=='outer_member', 'gene_symbols'])))\n",
    "            pathway_ligands = set(self.vocab(pathway_ligands))\n",
    "        else:\n",
    "            pathway_ligands = set(self.vocab(list(df_label.loc[df_label['class']=='outer_member', 'gene_symbols'])))\n",
    "        \n",
    "        niche_ligands_expression = self.adata.obsm['niche_ligands_expression'].A\n",
    "        for i in tqdm(range(niche_ligands_expression.shape[0])):\n",
    "            if len(set(self.ligand_ids[np.nonzero(niche_ligands_expression[i])[0]]).intersection(pathway_ligands))>=0:\n",
    "                selected_cell_index.append(i)\n",
    "\n",
    "        gexpr_feature = self.adata.X[selected_cell_index].A\n",
    "        S = gexpr_feature.sum(1)\n",
    "        T = S\n",
    "        TS = np.concatenate([[np.log10(T)],[np.log10(S)]],axis=0).T\n",
    "        data = np.concatenate([gexpr_feature,TS],axis=1)\n",
    "        self.data = data\n",
    "        print(f\"Selected cell number: {len(selected_cell_index): 6d}/{self.adata.shape[0]: 6d}\")\n",
    "\n",
    "        member_genes = df_label.loc[df_label['class']=='inner_member', 'gene_symbols'].values.tolist()\n",
    "        random.seed(0)\n",
    "        nonmember_genes = random.sample(sorted(set(selected_gene_names).difference(set(df_label['gene_symbols']))), len(member_genes))\n",
    "        gene_targets = member_genes + nonmember_genes\n",
    "        gene_targets = np.array(self.vocab(gene_targets))\n",
    "        gene_labels = [1]*len(member_genes)+[0]*len(nonmember_genes)\n",
    "        \n",
    "        gene2label = dict(zip(gene_targets, gene_labels))\n",
    "        for g in self.gene_ids:\n",
    "            if g not in gene2label:\n",
    "                gene2label[g] = -100\n",
    "        gene2label[19264] = gene2label[19265] = gene2label[19266] = -100\n",
    "\n",
    "        self.gene_targets = gene_targets\n",
    "        self.gene_labels = gene_labels\n",
    "        self.gene2label = gene2label\n",
    "\n",
    "    def prepare_train_and_valid_data(self, train_index, valid_index):\n",
    "        gene_targets_train = self.gene_targets[train_index]\n",
    "        gene_targets_valid = self.gene_targets[valid_index]\n",
    "\n",
    "        gene2label_train = self.gene2label.copy()\n",
    "        for gene in gene2label_train:\n",
    "            if gene not in gene_targets_train:\n",
    "                gene2label_train[gene] = -100\n",
    "        gene2label_valid = self.gene2label.copy()\n",
    "        for gene in gene2label_valid:\n",
    "            if gene not in gene_targets_valid:\n",
    "                gene2label_valid[gene] = -100\n",
    "\n",
    "        data = self.data\n",
    "        train_data = [d for d in data if len(set(np.nonzero(d)[0]).intersection(set(gene_targets_train)))>0]\n",
    "        valid_data = [d for d in data if len(set(np.nonzero(d)[0]).intersection(set(gene_targets_valid)))>0]\n",
    "        train_data = torch.from_numpy(np.array(train_data)).float()\n",
    "        valid_data = torch.from_numpy(np.array(valid_data)).float()\n",
    "        train_data_gene_ids = torch.arange(train_data.shape[1]).repeat(train_data.shape[0], 1)\n",
    "        valid_data_gene_ids = torch.arange(valid_data.shape[1]).repeat(valid_data.shape[0], 1)\n",
    "\n",
    "        train_data_index = train_data != 0\n",
    "        train_values, train_padding = load.gatherData(train_data, train_data_index, self.pad_value)\n",
    "        train_gene_ids, _ = load.gatherData(train_data_gene_ids, train_data_index, self.pad_token)\n",
    "        train_gene_labels = torch.from_numpy(np.vectorize(gene2label_train.get)(train_gene_ids.numpy()))\n",
    "        train_data = {'values': train_values, 'padding': train_padding, 'gene_ids': train_gene_ids, 'gene_labels': train_gene_labels}\n",
    "\n",
    "        valid_data_index = valid_data != 0\n",
    "        valid_values, valid_padding = load.gatherData(valid_data, valid_data_index, self.pad_value)\n",
    "        valid_gene_ids, _ = load.gatherData(valid_data_gene_ids, valid_data_index, self.pad_token)\n",
    "        valid_gene_labels = torch.from_numpy(np.vectorize(gene2label_valid.get)(valid_gene_ids.numpy()))\n",
    "        valid_data = {'values': valid_values, 'padding': valid_padding, 'gene_ids': valid_gene_ids, 'gene_labels': valid_gene_labels}\n",
    "\n",
    "        return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scF_Gcl(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            scf_token_emb,\n",
    "            scf_pos_emb,\n",
    "            scf_encoder,\n",
    "            d_model: int,\n",
    "            n_gcl: int = 2,\n",
    "            nlayers_gcl: int = 3,\n",
    "    ):\n",
    "        super(scF_Gcl, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.token_emb = scf_token_emb\n",
    "        self.pos_emb = scf_pos_emb\n",
    "        self.encoder = scf_encoder\n",
    "        self.gcl_decoder = GclDecoder(d_model, n_gcl, nlayers=nlayers_gcl)\n",
    "\n",
    "    def forward(self, gene_values, padding_label, gene_ids):\n",
    "\n",
    "        # token and positional embedding\n",
    "        x = self.token_emb(torch.unsqueeze(gene_values, 2), output_weight = 0)\n",
    "\n",
    "        position_emb = self.pos_emb(gene_ids)\n",
    "        x += position_emb\n",
    "        x = self.encoder(x, padding_mask=padding_label)\n",
    "        output = self.gcl_decoder(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class GclDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder for gene classification task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_gcl: int,\n",
    "        nlayers: int = 3,\n",
    "        activation: callable = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # module list\n",
    "        self._decoder = nn.ModuleList()\n",
    "        for i in range(nlayers - 1):\n",
    "            self._decoder.append(nn.Linear(d_model, d_model))\n",
    "            self._decoder.append(activation())\n",
    "            self._decoder.append(nn.LayerNorm(d_model))\n",
    "        self.out_layer = nn.Linear(d_model, n_gcl)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embsize]\n",
    "        \"\"\"\n",
    "        for layer in self._decoder:\n",
    "            x = layer(x)\n",
    "        return self.out_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model: nn.Module, train_data, batch_size, scaler, optimizer, scheduler, log_interval, epoch) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    amp = True\n",
    "\n",
    "    model.train()\n",
    "    total_gcl = 0.0\n",
    "    total_error = 0.0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_values = train_data['values']\n",
    "    train_padding = train_data['padding']\n",
    "    train_gene_ids = train_data['gene_ids']\n",
    "    train_gene_labels = train_data['gene_labels']\n",
    "\n",
    "    num_batches = np.ceil(len(train_values)/batch_size).astype(int)\n",
    "    for k in range(0, len(train_values), batch_size):\n",
    "        batch = int(k/batch_size+1)\n",
    "        if k+batch_size>len(train_values):\n",
    "            break\n",
    "        with torch.cuda.amp.autocast(enabled=amp):\n",
    "            output = model(train_values[k:k+batch_size].to(device), \n",
    "                           train_padding[k:k+batch_size].to(device), \n",
    "                           train_gene_ids[k:k+batch_size].to(device))\n",
    "            \n",
    "            batch_train_gene_labels = train_gene_labels[k:k+batch_size].to(device)\n",
    "            batch_logits = output[torch.logical_or(batch_train_gene_labels==1, batch_train_gene_labels==0)]\n",
    "            batch_labels = batch_train_gene_labels[torch.logical_or(batch_train_gene_labels==1, batch_train_gene_labels==0)]\n",
    "            targets = train_gene_ids[k:k+batch_size].to(device)[torch.logical_or(batch_train_gene_labels==1, batch_train_gene_labels==0)]\n",
    "            loss_gcl = 0.0\n",
    "            for t in set(targets):\n",
    "                loss_gcl += criterion_cls(batch_logits[targets==t], batch_labels[targets==t])\n",
    "            loss_gcl = loss_gcl/len(set(targets))\n",
    "\n",
    "            error_rate_gcl = 1 - (\n",
    "                    (batch_logits.argmax(1) == batch_labels)\n",
    "                    .sum()\n",
    "                    .item()\n",
    "                ) / batch_labels.size(0)\n",
    "            \n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss_gcl).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_gcl += loss_gcl.item()\n",
    "        total_error += error_rate_gcl\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            sec_per_batch = (time.time() - start_time) / log_interval\n",
    "            cur_gcl = total_gcl / log_interval\n",
    "            cur_error = total_error / log_interval\n",
    "            print(f\"| Split {split} | epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.8f} | sec/batch {sec_per_batch:5.1f} | \"\n",
    "                f\"gcl {cur_gcl:5.5f} | \"\n",
    "                f\"acc {1-cur_error:1.5f} | \"\n",
    "            )\n",
    "            total_gcl = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def py_softmax(vector):\n",
    "\te = np.exp(vector)\n",
    "\treturn e / e.sum()\n",
    "\n",
    "def evaluate(model: nn.Module, valid_data, batch_size, curve) -> None:\n",
    "    amp = True\n",
    "    \n",
    "    model.eval()\n",
    "    total_gcl = 0.0\n",
    "    total_error = 0.0\n",
    "    total_num = 0\n",
    "    \n",
    "    logits = []\n",
    "    labels = []\n",
    "\n",
    "    valid_values = valid_data['values']\n",
    "    valid_padding = valid_data['padding']\n",
    "    valid_gene_ids = valid_data['gene_ids']\n",
    "    valid_gene_labels = valid_data['gene_labels']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for k in tqdm(range(0, len(valid_values), batch_size)):\n",
    "            if k+batch_size>len(valid_values):\n",
    "                break\n",
    "            with torch.cuda.amp.autocast(enabled=amp):\n",
    "                output = model(valid_values[k:k+batch_size].to(device), \n",
    "                           valid_padding[k:k+batch_size].to(device), \n",
    "                           valid_gene_ids[k:k+batch_size].to(device))\n",
    "            \n",
    "                batch_valid_gene_labels = valid_gene_labels[k:k+batch_size].to(device)\n",
    "                batch_logits = output[torch.logical_or(batch_valid_gene_labels==1, batch_valid_gene_labels==0)]\n",
    "                batch_labels = batch_valid_gene_labels[torch.logical_or(batch_valid_gene_labels==1, batch_valid_gene_labels==0)]\n",
    "                logits.append(batch_logits.to('cpu'))\n",
    "                labels.append(batch_labels.to('cpu'))\n",
    "            \n",
    "            accuracy = (batch_logits.argmax(1) == batch_labels).sum().item()\n",
    "            total_error += batch_labels.size(0) - accuracy\n",
    "            total_num += batch_labels.size(0)\n",
    "            total_gcl += criterion_cls(batch_logits, batch_labels).item()*batch_labels.size(0) \n",
    "\n",
    "    logits = torch.concat(logits).float()\n",
    "    labels = torch.concat(labels)\n",
    "\n",
    "    y_score = [py_softmax(item)[1] for item in logits.numpy()]\n",
    "    y_true = labels.numpy()\n",
    "\n",
    "    if curve == 'roc':\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        auc_value = auc(fpr, tpr)\n",
    "    elif curve == 'prc':\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "        auc_value = auc(recall, precision)\n",
    "\n",
    "    val_err = total_error / total_num\n",
    "    val_loss = total_gcl / total_num\n",
    "    print(\"-\" * 89)\n",
    "    print(\n",
    "        f\"valid accuracy: {1-val_err:1.4f} | \"\n",
    "        f\"valid auc: {auc_value:1.4f} | \"\n",
    "        f\"valid loss: {val_loss:1.4f} | \"\n",
    "    )\n",
    "    print(\"-\" * 89)\n",
    "\n",
    "    if curve == 'roc':\n",
    "        return auc_value, fpr, tpr\n",
    "    elif curve == 'prc':\n",
    "        return auc_value, recall, precision\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_loader, valid_loader, batch_size, epochs, mean_curve_x, curve):\n",
    "    lr = 1e-4\n",
    "    amp = True\n",
    "    schedule_ratio = 0.9\n",
    "    schedule_interval = 1\n",
    "    log_interval = 10\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=lr, eps=1e-4 if amp else 1e-8\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, schedule_interval, gamma=schedule_ratio\n",
    "    )\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "    best_val_auc = 0\n",
    "    best_curve_x = 0\n",
    "    best_curve_y = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train(model, train_loader, batch_size, scaler, optimizer, scheduler, log_interval, epoch)\n",
    "\n",
    "        val_auc, curve_x, curve_y = evaluate(model, valid_loader, batch_size, curve)\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_curve_x = curve_x\n",
    "            best_curve_y = curve_y\n",
    "            print(f\"Best model with auc {best_val_auc:1.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "    \n",
    "    if curve == 'roc':\n",
    "        interp_curve_y = np.interp(mean_curve_x, best_curve_x, best_curve_y)\n",
    "        interp_curve_y[0] = 0.0\n",
    "    elif curve == 'prc':\n",
    "        interp_curve_y = np.interp(mean_curve_x, best_curve_x[::-1], best_curve_y[::-1])\n",
    "        interp_curve_y[0] = 1.0\n",
    "    \n",
    "    return interp_curve_y, best_curve_x, best_curve_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scFoundation(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            scf_token_emb,\n",
    "            scf_pos_emb,\n",
    "            scf_encoder,\n",
    "            scf_decoder,\n",
    "            scf_decoder_embed,\n",
    "            scf_norm,\n",
    "            scf_to_final,\n",
    "    ):\n",
    "        super(scFoundation, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.token_emb = scf_token_emb\n",
    "        self.pos_emb = scf_pos_emb\n",
    "\n",
    "        # ## DEBUG\n",
    "        self.encoder = scf_encoder\n",
    "\n",
    "        ##### decoder\n",
    "        self.decoder = scf_decoder\n",
    "        self.decoder_embed = scf_decoder_embed\n",
    "        self.norm = scf_norm\n",
    "        self.to_final = scf_to_final\n",
    "\n",
    "    def forward(self, x, padding_label, encoder_position_gene_ids, encoder_labels, decoder_data,\n",
    "                decoder_position_gene_ids, decoder_data_padding_labels, **kwargs):\n",
    "\n",
    "        # token and positional embedding\n",
    "        x = self.token_emb(torch.unsqueeze(x, 2), output_weight = 0)\n",
    "\n",
    "        position_emb = self.pos_emb(encoder_position_gene_ids)\n",
    "        x += position_emb\n",
    "        x = self.encoder(x, padding_mask=padding_label)\n",
    "\n",
    "        decoder_data = self.token_emb(torch.unsqueeze(decoder_data, 2))\n",
    "        position_emb = self.pos_emb(decoder_position_gene_ids)\n",
    "        batch_idx, gen_idx = (encoder_labels == True).nonzero(as_tuple=True)\n",
    "        decoder_data[batch_idx, gen_idx] = x[~padding_label].to(decoder_data.dtype)\n",
    "\n",
    "        decoder_data += position_emb\n",
    "\n",
    "        decoder_data = self.decoder_embed(decoder_data)\n",
    "        x = self.decoder(decoder_data, padding_mask=decoder_data_padding_labels)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        # return x\n",
    "        x = self.to_final(x)\n",
    "        return x.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_file):\n",
    "    if model_file is None:\n",
    "        # load pretrained model\n",
    "        pretrainmodel, pretrainconfig = load.load_model_frommmf('scfoundation/models/models.ckpt')\n",
    "    else:\n",
    "        # load fine-tuned model\n",
    "        pretrainmodel = torch.load(f'fine-tuning_mse/models/{model_file}', map_location='cpu')\n",
    "        pretrainmodel = pretrainmodel.module\n",
    "    \n",
    "    model = scF_Gcl(pretrainmodel.token_emb,\n",
    "            pretrainmodel.pos_emb,\n",
    "            pretrainmodel.encoder,\n",
    "            d_model = 768,\n",
    "            n_gcl = 2,\n",
    "            nlayers_gcl = 3\n",
    "            )\n",
    "    \n",
    "    pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "    for name, para in model.named_parameters():\n",
    "        para.requires_grad = False\n",
    "    for name, para in model.encoder.transformer_encoder[10:12].named_parameters():\n",
    "        para.requires_grad = True\n",
    "    for name, para in model.gcl_decoder.named_parameters():\n",
    "        para.requires_grad = True\n",
    "    post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "    print(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "    print(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = 19266\n",
    "pad_value = 103\n",
    "tokenizer_dir = '../stformer/tokenizer/'\n",
    "vocab_file = tokenizer_dir + \"scfoundation_gene_vocab.json\"\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "vocab.append_token(\"<pad>\")\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "\n",
    "curve = 'prc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = None\n",
    "label_path = 'gene_lists/TGF-beta_signaling_pathway_KEGG.csv'\n",
    "pathway_ligands = ['BMP2','BMP4','BMP5','BMP6','BMP7','BMP8A','BMP8B','GDF5','GDF6','GDF7','INHBA','INHBB','INHBC','INHBE','TGFB1','TGFB2','TGFB3','NODAL']\n",
    "\n",
    "dataset = 'human_myocardial_infarction'\n",
    "slide = 'ACH005'\n",
    "adata = sc.read_h5ad(f'../datasets/{slide}_niche.h5ad')\n",
    "\n",
    "# selected_celltype = 'Fibroblast'\n",
    "# adata = adata[adata.obs['cell_type']==list(adata.uns['cell_types_list']).index(selected_celltype)+1]\n",
    "np.random.seed(0)\n",
    "shuffled_indices = np.random.permutation(adata.n_obs)\n",
    "adata = adata[shuffled_indices]\n",
    "# adata = adata[:2000]\n",
    "\n",
    "tokenizer = Tokenizer(tokenizer_dir, adata, label_path, vocab, pad_value, pad_token, pathway_ligands)\n",
    "tokenizer.prepare_data()\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 30\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "mean_curve_x = np.linspace(0, 1, 100)\n",
    "all_curve_y = []\n",
    "all_auc = []\n",
    "all_wt = []\n",
    "\n",
    "split = 0\n",
    "for train_index, valid_index in skf.split(tokenizer.gene_targets, tokenizer.gene_labels):\n",
    "    split += 1\n",
    "    print(f\"Cross-validate on dataset {dataset} slide {slide} - split {split}\")\n",
    "    train_data, valid_data = tokenizer.prepare_train_and_valid_data(train_index, valid_index)\n",
    "\n",
    "    model = initialize_model(model_file)\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0, 1, 2])\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model.to(device)\n",
    "\n",
    "    interp_curve_y, best_curve_x, best_curve_y = train_and_evaluate(model, train_data, valid_data, batch_size, epochs, mean_curve_x, curve)\n",
    "    all_curve_y.append(interp_curve_y)\n",
    "    all_auc.append(auc(best_curve_x, best_curve_y))\n",
    "    all_wt.append(len(best_curve_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def get_cross_valid_metrics(all_curve_y, all_auc, all_wt, curve):\n",
    "    wts = [count/sum(all_wt) for count in all_wt]\n",
    "    print(wts)\n",
    "    all_weighted_curve_y = [a*b for a,b in zip(all_curve_y, wts)]\n",
    "    mean_curve_y = np.sum(all_weighted_curve_y, axis=0)\n",
    "    if curve == 'roc':\n",
    "        mean_curve_y[-1] = 1.0\n",
    "    all_weighted_auc = [a*b for a,b in zip(all_auc, wts)]\n",
    "    auc_mean = np.sum(all_weighted_auc)\n",
    "    auc_sd = math.sqrt(np.average((all_auc-auc_mean)**2, weights=wts))\n",
    "    return mean_curve_y, auc_mean, auc_sd, wts\n",
    "\n",
    "mean_curve_y, auc_mean, auc_sd, wts = get_cross_valid_metrics(all_curve_y, all_auc, all_wt, curve)\n",
    "\n",
    "print(f\"Mean AUC: {auc_mean} +/- {auc_sd}\")\n",
    "cv_results = {'auc_mean':auc_mean, 'auc_sd':auc_sd, 'mean_curve_x':mean_curve_x, 'mean_curve_y':mean_curve_y, 'all_auc':all_auc, 'wts':wts, 'all_curve_y':all_curve_y}\n",
    "pickle.dump(cv_results, open(f'figures/gene_classification/scf_gcl_{slide}_nfkb_{curve}.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ROC(bundled_data, title, curve):\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    for auc_mean, auc_sd, mean_curve_x, mean_curve_y, sample, color in bundled_data:\n",
    "        plt.plot(mean_curve_x, mean_curve_y, color=color,\n",
    "                 lw=lw, label=\"{0} (AUC {1:0.3f} $\\pm$ {2:0.3f})\".format(sample, auc_mean, auc_sd))\n",
    "    if curve == 'roc':\n",
    "        plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "    elif curve == 'prc':\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "bundled_data = [(auc_mean, auc_sd, mean_curve_x, mean_curve_y, \"scFoundation\", \"red\")]\n",
    "\n",
    "plot_ROC(bundled_data, 'Gene classification', curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stformer",
   "language": "python",
   "name": "stformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
