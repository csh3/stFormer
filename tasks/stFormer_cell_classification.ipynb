{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "from typing import Dict\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import auc, roc_curve, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from stformer import logger\n",
    "from stformer.tokenizer import GeneVocab\n",
    "from stformer.tokenizer import tokenize_and_pad_batch_2\n",
    "from stformer.model import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, tokenizer_dir, adata, vocab, pad_value, pad_token):\n",
    "        self.tokenizer_dir = tokenizer_dir\n",
    "        self.adata = adata\n",
    "        self.vocab = vocab\n",
    "        self.pad_value = pad_value\n",
    "        self.pad_token = pad_token\n",
    "        self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        self.expression_matrix = self.adata.X.A\n",
    "        self.niche_ligands_expression = self.adata.obsm['niche_ligands_expression'].A\n",
    "        self.niche_composition = self.adata.obsm['niche_composition'].A\n",
    "\n",
    "        gene_list_df = pd.read_csv(f'{self.tokenizer_dir}/OS_scRNA_gene_index.19264.tsv', header=0, delimiter='\\t')\n",
    "        gene_list = list(gene_list_df['gene_name'])\n",
    "        self.gene_ids = np.array(self.vocab(gene_list), dtype=int)\n",
    "\n",
    "        ligand_database = pd.read_csv(self.tokenizer_dir+'ligand_database.csv', header=0, index_col=0)\n",
    "        ligand_symbol = ligand_database[ligand_database.sum(1)>1].index.values\n",
    "        ligand_symbol = gene_list_df.loc[gene_list_df['gene_name'].isin(ligand_symbol), 'gene_name'].values\n",
    "        self.ligand_ids = np.array(self.vocab(ligand_symbol.tolist())*50, dtype=int)\n",
    "\n",
    "        self.celltype_labels = self.adata.obs['cell_type'].cat.codes.values\n",
    "\n",
    "    def tokenize_data(self, train_index, valid_index):\n",
    "        biases = np.zeros([self.niche_composition.shape[0], self.niche_composition.shape[1]*986])\n",
    "        for k in range(biases.shape[0]):\n",
    "            biases[k] = np.concatenate([[np.log(p)]*986 for p in self.niche_composition[k]])\n",
    "\n",
    "        self.celltype_labels_train = torch.from_numpy(self.celltype_labels[train_index]).long()\n",
    "        self.celltype_labels_valid = torch.from_numpy(self.celltype_labels[valid_index]).long()\n",
    "\n",
    "        train_expression = self.expression_matrix[train_index]\n",
    "        train_niche = self.niche_ligands_expression[train_index]\n",
    "        train_biases = biases[train_index]\n",
    "        \n",
    "        valid_expression = self.expression_matrix[valid_index]\n",
    "        valid_niche = self.niche_ligands_expression[valid_index]\n",
    "        valid_biases = biases[valid_index]\n",
    "\n",
    "        tokenized_train = tokenize_and_pad_batch_2(\n",
    "            train_expression,\n",
    "            train_niche,\n",
    "            train_biases,\n",
    "            self.gene_ids,\n",
    "            self.ligand_ids,\n",
    "            pad_id = self.vocab[self.pad_token],\n",
    "            pad_value = self.pad_value,\n",
    "        )\n",
    "\n",
    "        tokenized_valid = tokenize_and_pad_batch_2(\n",
    "            valid_expression,\n",
    "            valid_niche,\n",
    "            valid_biases,\n",
    "            self.gene_ids,\n",
    "            self.ligand_ids,\n",
    "            pad_id = self.vocab[self.pad_token],\n",
    "            pad_value = self.pad_value,\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f\"train set number of samples: {tokenized_train['center_genes'].shape[0]}, \"\n",
    "            f\"\\n\\t feature length of center cell: {tokenized_train['center_genes'].shape[1]}\"\n",
    "            f\"\\n\\t feature length of niche cells: {tokenized_train['niche_genes'].shape[1]}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"valid set number of samples: {tokenized_valid['center_genes'].shape[0]}, \"\n",
    "            f\"\\n\\t feature length of center cell: {tokenized_valid['center_genes'].shape[1]}\"\n",
    "            f\"\\n\\t feature length of niche cells: {tokenized_valid['niche_genes'].shape[1]}\"\n",
    "        )\n",
    "\n",
    "        self.tokenized_train = tokenized_train\n",
    "        self.tokenized_valid = tokenized_valid\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train_data_pt = {\n",
    "            \"center_gene_ids\": self.tokenized_train[\"center_genes\"],\n",
    "            \"input_center_values\": self.tokenized_train[\"center_values\"],\n",
    "            \"niche_gene_ids\": self.tokenized_train[\"niche_genes\"],\n",
    "            \"input_niche_values\": self.tokenized_train[\"niche_values\"],\n",
    "            \"cross_attn_bias\": self.tokenized_train[\"cross_attn_bias\"],\n",
    "            \"celltype_labels\": self.celltype_labels_train,\n",
    "        }\n",
    "\n",
    "        self.valid_data_pt = {\n",
    "            \"center_gene_ids\": self.tokenized_valid[\"center_genes\"],\n",
    "            \"input_center_values\": self.tokenized_valid[\"center_values\"],\n",
    "            \"niche_gene_ids\": self.tokenized_valid[\"niche_genes\"],\n",
    "            \"input_niche_values\": self.tokenized_valid[\"niche_values\"],\n",
    "            \"cross_attn_bias\": self.tokenized_valid[\"cross_attn_bias\"],\n",
    "            \"celltype_labels\": self.celltype_labels_valid,\n",
    "        }\n",
    "\n",
    "    def prepare_dataloader(self, batch_size):\n",
    "        train_loader = DataLoader(\n",
    "            dataset=SeqDataset(self.train_data_pt),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=min(len(os.sched_getaffinity(0)), batch_size // 2),\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        valid_loader = DataLoader(\n",
    "            dataset=SeqDataset(self.valid_data_pt),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=min(len(os.sched_getaffinity(0)), batch_size // 2),\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return train_loader, valid_loader\n",
    "\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"center_gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "    \n",
    "    \n",
    "def downsample(adata, n):\n",
    "    celltype_counts = adata.obs['cell_type'].value_counts() \n",
    "    to_downsample = celltype_counts[celltype_counts > 400].index.tolist()\n",
    "\n",
    "    keep_cells = np.zeros(len(adata), dtype=bool)\n",
    "    for ct in adata.obs['cell_type'].unique():\n",
    "        ct_mask = adata.obs['cell_type'] == ct\n",
    "        if ct in to_downsample:\n",
    "            np.random.seed(0)\n",
    "            sampled_idx = np.random.choice(\n",
    "                np.where(ct_mask)[0],  \n",
    "                size=n,               \n",
    "                replace=False           \n",
    "            )\n",
    "            keep_cells[sampled_idx] = True  \n",
    "\n",
    "    adata = adata[keep_cells]\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model: nn.Module, loader: DataLoader, scaler, optimizer, scheduler, log_interval, epoch, mode) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    amp = True\n",
    "\n",
    "    model.train()\n",
    "    total_cls = 0.0\n",
    "    total_error = 0.0\n",
    " \n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        center_gene_ids = batch_data[\"center_gene_ids\"].to(device)\n",
    "        if center_gene_ids.size(0)<8:\n",
    "            continue\n",
    "        input_center_values = batch_data[\"input_center_values\"].to(device)\n",
    "        niche_gene_ids = batch_data[\"niche_gene_ids\"].to(device)\n",
    "        input_niche_values = batch_data[\"input_niche_values\"].to(device)\n",
    "        cross_attn_bias = batch_data[\"cross_attn_bias\"].to(device)\n",
    "        celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "\n",
    "        if mode == 'sp':\n",
    "            encoder_src_key_padding_mask = niche_gene_ids.eq(vocab[pad_token])\n",
    "        elif mode == 'sc':\n",
    "            encoder_src_key_padding_mask = torch.ones_like(niche_gene_ids, dtype=torch.bool).to(device)\n",
    "        decoder_src_key_padding_mask = center_gene_ids.eq(vocab[pad_token])\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=amp):\n",
    "            output_dict = model(\n",
    "                    niche_gene_ids,\n",
    "                    input_niche_values,\n",
    "                    encoder_src_key_padding_mask,\n",
    "                    center_gene_ids,\n",
    "                    input_center_values,\n",
    "                    decoder_src_key_padding_mask,\n",
    "                    cross_attn_bias,\n",
    "                    CLS = True,\n",
    "                )\n",
    "\n",
    "            loss_cls = criterion_cls(output_dict[\"cls_output\"], celltype_labels)\n",
    "\n",
    "            error_rate_cls = 1 - (\n",
    "                    (output_dict[\"cls_output\"].argmax(1) == celltype_labels)\n",
    "                    .sum()\n",
    "                    .item()\n",
    "                ) / celltype_labels.size(0)\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss_cls).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_cls += loss_cls.item()\n",
    "        total_error += error_rate_cls\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            sec_per_batch = (time.time() - start_time) / log_interval\n",
    "            cur_cls = total_cls / log_interval\n",
    "            cur_error = total_error / log_interval\n",
    "            logger.info(\n",
    "                f\"| Split {split} | {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.8f} | sec/batch {sec_per_batch:5.1f} | \"\n",
    "                f\"cls {cur_cls:5.5f} | \"\n",
    "                f\"acc {1-cur_error:1.5f} | \"\n",
    "            )\n",
    "            total_cls = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, mode) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    amp = True\n",
    "    \n",
    "    model.eval()\n",
    "    total_cls = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_num = 0\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    # batch_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(loader):\n",
    "            # batch_num += 1\n",
    "            # if batch_num>100:\n",
    "            #     break\n",
    "            center_gene_ids = batch_data[\"center_gene_ids\"].to(device)\n",
    "            if center_gene_ids.size(0)<8:\n",
    "                continue\n",
    "            input_center_values = batch_data[\"input_center_values\"].to(device)\n",
    "            niche_gene_ids = batch_data[\"niche_gene_ids\"].to(device)\n",
    "            input_niche_values = batch_data[\"input_niche_values\"].to(device)\n",
    "            cross_attn_bias = batch_data[\"cross_attn_bias\"].to(device)\n",
    "            celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "\n",
    "            if mode == 'sp':\n",
    "                encoder_src_key_padding_mask = niche_gene_ids.eq(vocab[pad_token])\n",
    "            elif mode == 'sc':\n",
    "                encoder_src_key_padding_mask = torch.ones_like(niche_gene_ids, dtype=torch.bool).to(device)\n",
    "            decoder_src_key_padding_mask = center_gene_ids.eq(vocab[pad_token])\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=amp):\n",
    "                output_dict = model(\n",
    "                        niche_gene_ids,\n",
    "                        input_niche_values,\n",
    "                        encoder_src_key_padding_mask,\n",
    "                        center_gene_ids,\n",
    "                        input_center_values,\n",
    "                        decoder_src_key_padding_mask,\n",
    "                        cross_attn_bias,\n",
    "                        CLS = True,\n",
    "                    )\n",
    "                \n",
    "                loss_cls = criterion_cls(output_dict[\"cls_output\"], celltype_labels)\n",
    "                \n",
    "            accuracy = (output_dict[\"cls_output\"].argmax(1) == celltype_labels).sum().item()\n",
    "            total_accuracy += accuracy\n",
    "            total_num += celltype_labels.size(0)\n",
    "            total_cls += loss_cls*celltype_labels.size(0)\n",
    "\n",
    "            true_labels.append(celltype_labels.to('cpu'))\n",
    "            predicted_labels.append(output_dict[\"cls_output\"].argmax(1).to('cpu'))\n",
    "\n",
    "    val_accuracy = total_accuracy / total_num\n",
    "    val_loss = total_cls / total_num\n",
    "\n",
    "    true_labels = torch.cat(true_labels).numpy()\n",
    "    predicted_labels = torch.cat(predicted_labels).numpy()\n",
    "    \n",
    "    return val_loss, val_accuracy, true_labels, predicted_labels\n",
    "\n",
    "def train_and_evaluate(model, train_loader, valid_loader, epochs, mode):\n",
    "    lr = 1e-4\n",
    "    amp = True\n",
    "    schedule_ratio = 0.9\n",
    "    schedule_interval = 1\n",
    "    log_interval = 10\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=lr, eps=1e-4 if amp else 1e-8\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, schedule_interval, gamma=schedule_ratio\n",
    "    )\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "    best_val_accuracy = 0\n",
    "    best_model = None\n",
    "    best_true_labels = None\n",
    "    best_predicted_labels = None\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train(model, train_loader, scaler, optimizer, scheduler, log_interval, epoch, mode)\n",
    "\n",
    "        val_loss, val_accuracy, true_labels, predicted_labels = evaluate(model, valid_loader, mode)\n",
    "        logger.info(\"-\" * 89)\n",
    "        logger.info(\n",
    "            f\"| end of epoch {epoch:3d} | \"\n",
    "            f\"valid accuracy: {val_accuracy:1.4f} | \"\n",
    "            f\"valid loss: {val_loss:5.4f} | \"\n",
    "        )\n",
    "        logger.info(\"-\" * 89)\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            logger.info(f\"Best model with accuracy {best_val_accuracy:1.4f}\")\n",
    "            \n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_true_labels = true_labels\n",
    "            best_predicted_labels = predicted_labels\n",
    "\n",
    "        scheduler.step()\n",
    "    \n",
    "    return best_val_accuracy, best_model, best_true_labels, best_predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scfoundation import load\n",
    "\n",
    "def initialize_model(model_file, n_cls):\n",
    "    pretrainmodel, pretrainconfig = load.load_model_frommmf('scfoundation/models/models.ckpt')\n",
    "\n",
    "    model = TransformerModel(\n",
    "        embsize,\n",
    "        nhead,\n",
    "        d_hid,\n",
    "        nlayers,\n",
    "        do_cls = True,\n",
    "        nlayers_cls = 3,\n",
    "        n_cls = n_cls,\n",
    "        dropout = dropout,\n",
    "        cell_emb_style = cell_emb_style,\n",
    "        scfoundation_token_emb1 = copy.deepcopy(pretrainmodel.token_emb),\n",
    "        scfoundation_token_emb2 = copy.deepcopy(pretrainmodel.token_emb),\n",
    "        scfoundation_pos_emb1 = copy.deepcopy(pretrainmodel.pos_emb),\n",
    "        scfoundation_pos_emb2 = copy.deepcopy(pretrainmodel.pos_emb),\n",
    "    )\n",
    "\n",
    "    pt_model = torch.load(model_file, map_location='cpu')\n",
    "\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = pt_model.state_dict()\n",
    "    pretrained_dict = {\n",
    "                k: v\n",
    "                for k, v in pretrained_dict.items()\n",
    "                if 'cls_decoder' not in k and 'gcl_decoder' not in k\n",
    "                # if k in model_dict and v.shape == model_dict[k].shape\n",
    "    }\n",
    "    # for k, v in pretrained_dict.items():\n",
    "    #     logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "    for name, para in model.named_parameters():\n",
    "        para.requires_grad = False\n",
    "    for name, para in model.transformer_decoder.layers[4:6].named_parameters():\n",
    "        para.requires_grad = True\n",
    "    for name, para in model.cls_decoder.named_parameters():\n",
    "        para.requires_grad = True\n",
    "    post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "    logger.info(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "    logger.info(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "\n",
    "    return model        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embsize = 768\n",
    "d_hid = 3072\n",
    "nhead = 12\n",
    "nlayers = 6\n",
    "dropout = 0.1\n",
    "cell_emb_style = 'max-pool'\n",
    "CLS = True\n",
    "mode = 'sp'\n",
    "\n",
    "pad_token = \"<pad>\"\n",
    "pad_value = 103\n",
    "tokenizer_dir = '../stformer/tokenizer/'\n",
    "vocab_file = tokenizer_dir + \"scfoundation_gene_vocab.json\"\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "vocab.append_token(pad_token)\n",
    "vocab.set_default_index(vocab[pad_token])\n",
    "\n",
    "model_file = '../pretraining/models/model_4.1M.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'pancreas_cosmx'\n",
    "adata = sc.read_h5ad(f'../datasets/pancreas_cosmx_niche.h5ad')\n",
    "\n",
    "adata.obs['cell_type'] = adata.obs['cell_type'].map(dict(zip(np.array(range(len(adata.uns['cell_types_list'])))+1, adata.uns['cell_types_list'])))\n",
    "adata.obs['cell_type'] = adata.obs['cell_type'].astype('category')\n",
    "\n",
    "adata = downsample(adata, 490)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_accuracy_dict={}\n",
    "cm_dict={}\n",
    "\n",
    "for max_niche_cell_num in [5,10,15,20,25,30,35]:\n",
    "\n",
    "    adata.obsm['niche_celltypes'] = adata.obsm[f'niche_celltypes_niche{max_niche_cell_num}']\n",
    "    adata.obsm['niche_composition'] = adata.obsm[f'niche_composition_niche{max_niche_cell_num}']\n",
    "    adata.obsm['niche_ligands_expression'] = adata.obsm[f'niche_ligands_expression_niche{max_niche_cell_num}']\n",
    "\n",
    "    tokenizer = Tokenizer(tokenizer_dir, adata, vocab, pad_value, pad_token)\n",
    "\n",
    "    epochs = 20\n",
    "    batch_size = 100\n",
    "\n",
    "    best_val_accuracy_list = []\n",
    "    best_true_labels_list = []\n",
    "    best_predicted_labels_list = []\n",
    "\n",
    "    n_splits = 5\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "\n",
    "    split = 0\n",
    "    for train_index, valid_index in skf.split(tokenizer.adata.obs_names.values, tokenizer.celltype_labels):\n",
    "        split += 1\n",
    "        logger.info(f\"Cross-validate on dataset {dataset} - split {split}\")\n",
    "        tokenizer.tokenize_data(train_index, valid_index)\n",
    "        tokenizer.prepare_data()\n",
    "        train_loader, valid_loader = tokenizer.prepare_dataloader(batch_size)\n",
    "        \n",
    "        model = initialize_model(model_file, len(set(tokenizer.celltype_labels)))\n",
    "        model = nn.DataParallel(model, device_ids = [0, 3, 1, 2])\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        model.to(device)\n",
    "\n",
    "        best_val_accuracy, best_model, best_true_labels, best_predicted_labels = train_and_evaluate(model, train_loader, valid_loader, epochs, mode)\n",
    "        best_val_accuracy_list.append(best_val_accuracy)\n",
    "        best_true_labels_list.append(best_true_labels)\n",
    "        best_predicted_labels_list.append(best_predicted_labels)\n",
    "\n",
    "    cm_list=[]\n",
    "    for i in range(n_splits):\n",
    "        cm = confusion_matrix(best_true_labels_list[i], best_predicted_labels_list[i], normalize='true')\n",
    "        cm_list.append(cm)\n",
    "    cm_dict[max_niche_cell_num] = cm_list\n",
    "\n",
    "    best_val_accuracy_dict[max_niche_cell_num] = best_val_accuracy_list\n",
    "\n",
    "    pickle.dump(best_val_accuracy_dict, open(f'figures/cell_classification/cv_accuracy_dict_4M.pkl', 'wb'))\n",
    "    pickle.dump(cm_dict, open(f'figures/cell_classification/cm_dict_4M.pkl', 'wb'))\n",
    "    pickle.dump(adata.obs['cell_type'].cat.categories.tolist(), open(f'figures/cell_classification/celltype_labels.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave out one FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'pancreas_cosmx'\n",
    "adata = sc.read_h5ad(f'../datasets/pancreas_cosmx_niche.h5ad')\n",
    "\n",
    "max_niche_cell_num = 20\n",
    "adata.obsm['niche_celltypes'] = adata.obsm[f'niche_celltypes_niche{max_niche_cell_num}']\n",
    "adata.obsm['niche_composition'] = adata.obsm[f'niche_composition_niche{max_niche_cell_num}']\n",
    "adata.obsm['niche_ligands_expression'] = adata.obsm[f'niche_ligands_expression_niche{max_niche_cell_num}']\n",
    "\n",
    "adata.obs['cell_type'] = adata.obs['cell_type'].map(dict(zip(np.array(range(len(adata.uns['cell_types_list'])))+1, adata.uns['cell_types_list'])))\n",
    "adata.obs['cell_type'] = adata.obs['cell_type'].astype('category')\n",
    "\n",
    "adata1 = adata[adata.obs['fov']!=52]\n",
    "adata1 = downsample(adata1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = Tokenizer(tokenizer_dir, adata1, vocab, pad_value, pad_token)\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "split = 0\n",
    "train_index, valid_index = train_test_split(np.array(range(tokenizer.adata1.shape[0])), test_size=0.2, random_state=42, stratify=tokenizer.celltype_labels)\n",
    "\n",
    "tokenizer.tokenize_data(train_index, valid_index)\n",
    "tokenizer.prepare_data()\n",
    "train_loader, valid_loader = tokenizer.prepare_dataloader(batch_size)\n",
    "    \n",
    "model = initialize_model(model_file, len(set(tokenizer.celltype_labels)))\n",
    "model = nn.DataParallel(model, device_ids = [2, 3, 1, 0])\n",
    "device = torch.device(\"cuda:2\")\n",
    "model.to(device)\n",
    "\n",
    "best_val_accuracy, best_model, best_true_labels, best_predicted_labels = train_and_evaluate(model, train_loader, valid_loader, epochs, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata2 = adata[adata.obs['fov']==52]\n",
    "adata3 = adata2[adata2.obs['cell_type'].isin(adata1.obs['cell_type'].cat.categories)]\n",
    "\n",
    "tokenizer = Tokenizer(tokenizer_dir, adata3, vocab, pad_value, pad_token)\n",
    "\n",
    "split = 0\n",
    "test_index = np.array(range(adata3.shape[0]))\n",
    "tokenizer.tokenize_data(test_index, test_index)\n",
    "tokenizer.prepare_data()\n",
    "test_loader, test_loader = tokenizer.prepare_dataloader(batch_size)\n",
    "\n",
    "test_loss, test_accuracy, true_labels, predicted_labels = evaluate(best_model, test_loader, mode)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true_labels, predicted_labels, normalize='true')\n",
    "pickle.dump(cm, open(f'figures/cell_classification/cm_fov52_4M-niche20.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata2.obs['predicted_celltypes'] = ['']*adata2.shape[0]\n",
    "predicted_celltypes = [adata3.obs['cell_type'].cat.categories[i] for i in predicted_labels]\n",
    "adata2.obs['predicted_celltypes'].loc[adata3.obs_names] = predicted_celltypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata2.write('figures/cell_classification/adata_fov52_4M-niche20.h5ad')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stformer",
   "language": "python",
   "name": "stformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
