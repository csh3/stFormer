{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "from typing import Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from stformer import logger\n",
    "from stformer.tokenizer import GeneVocab\n",
    "from stformer.tokenizer import tokenize_and_pad_batch_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, tokenizer_dir, adata, vocab, pad_value, pad_token):\n",
    "        self.tokenizer_dir = tokenizer_dir\n",
    "        self.adata = adata\n",
    "        self.vocab = vocab\n",
    "        self.pad_value = pad_value\n",
    "        self.pad_token = pad_token\n",
    "        self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        self.expression_matrix = self.adata.X.A\n",
    "        self.niche_ligands_expression = self.adata.obsm['niche_ligands_expression'].A\n",
    "        self.niche_composition = self.adata.obsm['niche_composition'].A\n",
    "\n",
    "        gene_list_df = pd.read_csv(f'{self.tokenizer_dir}/OS_scRNA_gene_index.19264.tsv', header=0, delimiter='\\t')\n",
    "        gene_list = list(gene_list_df['gene_name'])\n",
    "        self.gene_ids = np.array(self.vocab(gene_list), dtype=int)\n",
    "\n",
    "        ligand_database = pd.read_csv(self.tokenizer_dir+'ligand_database.csv', header=0, index_col=0)\n",
    "        ligand_symbol = ligand_database[ligand_database.sum(1)>1].index.values\n",
    "        ligand_symbol = gene_list_df.loc[gene_list_df['gene_name'].isin(ligand_symbol), 'gene_name'].values\n",
    "        self.ligand_ids = np.array(self.vocab(ligand_symbol.tolist())*25, dtype=int)\n",
    "\n",
    "    def tokenize_data(self):\n",
    "        biases = np.zeros([self.niche_composition.shape[0], self.niche_composition.shape[1]*986])\n",
    "        for k in range(biases.shape[0]):\n",
    "            biases[k] = np.concatenate([[np.log(p)]*986 for p in self.niche_composition[k]])\n",
    "\n",
    "        tokenized_data = tokenize_and_pad_batch_2(\n",
    "            self.expression_matrix,\n",
    "            self.niche_ligands_expression,\n",
    "            biases,\n",
    "            self.gene_ids,\n",
    "            self.ligand_ids,\n",
    "            pad_id = self.vocab[self.pad_token],\n",
    "            pad_value = self.pad_value,\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            f\"tokenize sample number: {tokenized_data['center_genes'].shape[0]}, \"\n",
    "            f\"\\n\\t feature length of center cell: {tokenized_data['center_genes'].shape[1]}\"\n",
    "            f\"\\n\\t feature length of niche cells: {tokenized_data['niche_genes'].shape[1]}\"\n",
    "        )\n",
    "\n",
    "        self.tokenized_data = tokenized_data\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.data_pt = {\n",
    "            \"center_gene_ids\": self.tokenized_data[\"center_genes\"],\n",
    "            \"input_center_values\": self.tokenized_data[\"center_values\"],\n",
    "            \"niche_gene_ids\": self.tokenized_data[\"niche_genes\"],\n",
    "            \"input_niche_values\": self.tokenized_data[\"niche_values\"],\n",
    "            \"cross_attn_bias\": self.tokenized_data[\"cross_attn_bias\"],\n",
    "        }\n",
    "    \n",
    "    def prepare_dataloader(self, batch_size):\n",
    "        data_loader = DataLoader(\n",
    "            dataset=SeqDataset(self.data_pt),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=min(len(os.sched_getaffinity(0)), batch_size // 2),\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"center_gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader, mode) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    cell_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(loader):\n",
    "            center_gene_ids = batch_data[\"center_gene_ids\"].to(device)\n",
    "            input_center_values = batch_data[\"input_center_values\"].to(device)\n",
    "            niche_gene_ids = batch_data[\"niche_gene_ids\"].to(device)\n",
    "            input_niche_values = batch_data[\"input_niche_values\"].to(device)\n",
    "            cross_attn_bias = batch_data[\"cross_attn_bias\"].to(device)\n",
    "\n",
    "            if mode == 'sp':\n",
    "                encoder_src_key_padding_mask = niche_gene_ids.eq(vocab[pad_token])\n",
    "            elif mode == 'sc':\n",
    "                encoder_src_key_padding_mask = torch.ones_like(niche_gene_ids, dtype=torch.bool).to(device)\n",
    "            decoder_src_key_padding_mask = center_gene_ids.eq(vocab[pad_token])\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=amp):\n",
    "                output_dict = model(\n",
    "                        niche_gene_ids,\n",
    "                        input_niche_values,\n",
    "                        encoder_src_key_padding_mask,\n",
    "                        center_gene_ids,\n",
    "                        input_center_values,\n",
    "                        decoder_src_key_padding_mask,\n",
    "                        cross_attn_bias,\n",
    "                    )\n",
    "                cell_embeddings.append(output_dict['cell_emb'].to('cpu'))\n",
    "    \n",
    "    return(torch.cat(cell_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embsize = 768\n",
    "d_hid = 3072\n",
    "nhead = 12\n",
    "nlayers = 6\n",
    "dropout = 0.1\n",
    "cell_emb_style = 'max-pool'\n",
    "\n",
    "from stformer.model import TransformerModel\n",
    "from tasks.scfoundation import load\n",
    "import copy\n",
    "\n",
    "pretrainmodel, pretrainconfig = load.load_model_frommmf('scfoundation/models/models.ckpt')\n",
    "\n",
    "model = TransformerModel(\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    dropout = dropout,\n",
    "    cell_emb_style = cell_emb_style,\n",
    "    scfoundation_token_emb1 = copy.deepcopy(pretrainmodel.token_emb),\n",
    "    scfoundation_token_emb2 = copy.deepcopy(pretrainmodel.token_emb),\n",
    "    scfoundation_pos_emb1 = copy.deepcopy(pretrainmodel.pos_emb),\n",
    "    scfoundation_pos_emb2 = copy.deepcopy(pretrainmodel.pos_emb),\n",
    ")\n",
    "\n",
    "del pretrainmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(f'../pretraining/models/model_4.1M.ckpt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\")\n",
    "\n",
    "model = nn.DataParallel(model, device_ids = [2, 0, 3, 1])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = \"<pad>\"\n",
    "pad_value = 103\n",
    "mode = 'sp'\n",
    "\n",
    "tokenizer_dir = '../stformer/tokenizer/'\n",
    "vocab_file = tokenizer_dir + \"scfoundation_gene_vocab.json\"\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "vocab.append_token(pad_token)\n",
    "vocab.set_default_index(vocab[pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide = '10X001'\n",
    "\n",
    "adata = sc.read_h5ad(f'../datasets/{slide}_niche.h5ad')\n",
    "\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "amp = True\n",
    "\n",
    "tokenizer = Tokenizer(tokenizer_dir, adata, vocab, pad_value, pad_token)\n",
    "tokenizer.tokenize_data()\n",
    "tokenizer.prepare_data()\n",
    "data_loader = tokenizer.prepare_dataloader(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_embeddings = evaluate(model, data_loader, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsm['cell_embeddings'] = cell_embeddings.numpy()\n",
    "adata.obs['cell_type'] = adata.obs['cell_type'].map(dict(zip(np.array(range(len(adata.uns['cell_types_list'])))+1, adata.uns['cell_types_list'])))\n",
    "adata.obs['cell_type'] = adata.obs['cell_type'].astype('category')\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata, use_rep='cell_embeddings')\n",
    "sc.tl.umap(adata)\n",
    "sc.pl.umap(adata,\n",
    "           title='',\n",
    "        #    frameon=False,\n",
    "        #    legend_loc='',\n",
    "        #    legend_fontsize='xx-small',\n",
    "           color='cell_type')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "plt.savefig(f'figures/cell_clustering/umap_4M_pt_{slide}.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scib\n",
    "\n",
    "adata.obs['batch'] = [1]*adata.shape[0]\n",
    "\n",
    "results = scib.metrics.metrics(\n",
    "        adata,\n",
    "        adata_int=adata,\n",
    "        label_key='cell_type',\n",
    "        batch_key='batch',\n",
    "        embed=\"cell_embeddings\",\n",
    "        silhouette_=True,\n",
    "        nmi_=True,  \n",
    "        ari_=True, \n",
    "    )\n",
    "\n",
    "result_dict = results[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stformer",
   "language": "python",
   "name": "stformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
