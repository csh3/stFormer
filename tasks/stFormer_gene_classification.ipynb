{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "from typing import Dict\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from stformer import logger\n",
    "from stformer.tokenizer import GeneVocab\n",
    "from stformer.tokenizer import tokenize_and_pad_batch_2\n",
    "from stformer.model import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, tokenizer_dir, adata, label_path, vocab, pad_value, pad_token, pathway_ligands=None):\n",
    "        self.tokenizer_dir = tokenizer_dir\n",
    "        self.adata = adata\n",
    "        self.label_path = label_path\n",
    "        self.vocab = vocab\n",
    "        self.pad_value = pad_value\n",
    "        self.pad_token = pad_token\n",
    "        self.pathway_ligands = pathway_ligands\n",
    "        self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        gene_list_df = pd.read_csv(f'{self.tokenizer_dir}/OS_scRNA_gene_index.19264.tsv', header=0, delimiter='\\t')\n",
    "        gene_list = list(gene_list_df['gene_name'])\n",
    "        self.gene_ids = np.array(self.vocab(gene_list), dtype=int)\n",
    "\n",
    "        ligand_database = pd.read_csv(self.tokenizer_dir+'ligand_database.csv', header=0, index_col=0)\n",
    "        ligand_symbol = ligand_database[ligand_database.sum(1)>1].index.values\n",
    "        ligand_symbol = gene_list_df.loc[gene_list_df['gene_name'].isin(ligand_symbol), 'gene_name'].values\n",
    "        self.ligand_ids = np.array(self.vocab(ligand_symbol.tolist())*self.niche_composition.shape[1], dtype=int)\n",
    "\n",
    "        df_label = pd.read_csv(f'{self.label_path}', header=0)\n",
    "        selected_gene_index = sc.pp.filter_genes(self.adata, min_cells=100, inplace=False)\n",
    "        selected_gene_names = self.adata.var_names.values[selected_gene_index[0]]\n",
    "        df_label = df_label[df_label['gene_symbols'].isin(selected_gene_names)]\n",
    "\n",
    "        selected_cell_index = []\n",
    "        if self.pathway_ligands is not None:\n",
    "            pathway_ligands = list(set(self.pathway_ligands).intersection(set(df_label.loc[df_label['class']=='outer_member', 'gene_symbols'])))\n",
    "            pathway_ligands = set(self.vocab(pathway_ligands))\n",
    "        else:\n",
    "            pathway_ligands = set(self.vocab(list(df_label.loc[df_label['class']=='outer_member', 'gene_symbols'])))\n",
    "        \n",
    "        niche_ligands_expression = self.adata.obsm['niche_ligands_expression'].A\n",
    "        for i in tqdm(range(niche_ligands_expression.shape[0])):\n",
    "            if len(set(self.ligand_ids[np.nonzero(niche_ligands_expression[i])[0]]).intersection(pathway_ligands))>=2:\n",
    "                selected_cell_index.append(i)\n",
    "\n",
    "        self.expression_matrix = self.adata.X[selected_cell_index].A\n",
    "        self.niche_ligands_expression = self.adata.obsm['niche_ligands_expression'][selected_cell_index].A\n",
    "        self.niche_composition = self.adata.obsm['niche_composition'][selected_cell_index].A\n",
    "        logger.info(f\"Selected cell number: {len(selected_cell_index): 6d}/{self.adata.shape[0]: 6d}\")\n",
    "\n",
    "        member_genes = df_label.loc[df_label['class']=='inner_member', 'gene_symbols'].values.tolist()\n",
    "        random.seed(0)\n",
    "        nonmember_genes = random.sample(sorted(set(selected_gene_names).difference(set(df_label['gene_symbols']))), len(member_genes))\n",
    "        gene_targets = member_genes + nonmember_genes\n",
    "        gene_targets = np.array(self.vocab(gene_targets))\n",
    "        gene_labels = [1]*len(member_genes)+[0]*len(nonmember_genes)\n",
    "        \n",
    "        gene2label = dict(zip(gene_targets, gene_labels))\n",
    "        for g in self.gene_ids:\n",
    "            if g not in gene2label:\n",
    "                gene2label[g] = -100\n",
    "        gene2label[self.vocab[self.pad_token]] = -100\n",
    "\n",
    "        self.gene_targets = gene_targets\n",
    "        self.gene_labels = gene_labels\n",
    "        self.gene2label = gene2label\n",
    "\n",
    "    def tokenize_data(self, train_index, valid_index):\n",
    "        biases = np.zeros([self.niche_composition.shape[0], self.niche_composition.shape[1]*986])\n",
    "        for k in range(biases.shape[0]):\n",
    "            biases[k] = np.concatenate([[np.log(p)]*986 for p in self.niche_composition[k]])\n",
    "\n",
    "        gene_targets_train = self.gene_targets[train_index]\n",
    "        gene_targets_valid = self.gene_targets[valid_index]\n",
    "        train_index = [k for k in range(self.expression_matrix.shape[0]) if len(set(self.gene_ids[np.nonzero(self.expression_matrix[k])[0]]).intersection(set(gene_targets_train)))>0]\n",
    "        valid_index = [k for k in range(self.expression_matrix.shape[0]) if len(set(self.gene_ids[np.nonzero(self.expression_matrix[k])[0]]).intersection(set(gene_targets_valid)))>0]\n",
    "\n",
    "        train_expression = self.expression_matrix[train_index]\n",
    "        train_niche = self.niche_ligands_expression[train_index]\n",
    "        train_biases = biases[train_index]\n",
    "        \n",
    "        valid_expression = self.expression_matrix[valid_index]\n",
    "        valid_niche = self.niche_ligands_expression[valid_index]\n",
    "        valid_biases = biases[valid_index]\n",
    "\n",
    "        tokenized_train = tokenize_and_pad_batch_2(\n",
    "            train_expression,\n",
    "            train_niche,\n",
    "            train_biases,\n",
    "            self.gene_ids,\n",
    "            self.ligand_ids,\n",
    "            pad_id = self.vocab[self.pad_token],\n",
    "            pad_value = self.pad_value,\n",
    "        )\n",
    "\n",
    "        tokenized_valid = tokenize_and_pad_batch_2(\n",
    "            valid_expression,\n",
    "            valid_niche,\n",
    "            valid_biases,\n",
    "            self.gene_ids,\n",
    "            self.ligand_ids,\n",
    "            pad_id = self.vocab[self.pad_token],\n",
    "            pad_value = self.pad_value,\n",
    "        )\n",
    "\n",
    "        gene2label_train = self.gene2label.copy()\n",
    "        for gene in gene2label_train:\n",
    "            if gene not in gene_targets_train:\n",
    "                gene2label_train[gene] = -100\n",
    "        gene2label_valid = self.gene2label.copy()\n",
    "        for gene in gene2label_valid:\n",
    "            if gene not in gene_targets_valid:\n",
    "                gene2label_valid[gene] = -100\n",
    "\n",
    "        tokenized_train['center_labels'] = torch.from_numpy(np.vectorize(gene2label_train.get)(tokenized_train['center_genes'].numpy()))\n",
    "        tokenized_valid['center_labels'] = torch.from_numpy(np.vectorize(gene2label_valid.get)(tokenized_valid['center_genes'].numpy()))\n",
    "        \n",
    "        logger.info(\n",
    "            f\"train set number of samples: {tokenized_train['center_genes'].shape[0]}, \"\n",
    "            f\"\\n\\t feature length of center cell: {tokenized_train['center_genes'].shape[1]}\"\n",
    "            f\"\\n\\t feature length of niche cells: {tokenized_train['niche_genes'].shape[1]}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"valid set number of samples: {tokenized_valid['center_genes'].shape[0]}, \"\n",
    "            f\"\\n\\t feature length of center cell: {tokenized_valid['center_genes'].shape[1]}\"\n",
    "            f\"\\n\\t feature length of niche cells: {tokenized_valid['niche_genes'].shape[1]}\"\n",
    "        )\n",
    "\n",
    "        self.tokenized_train = tokenized_train\n",
    "        self.tokenized_valid = tokenized_valid\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train_data_pt = {\n",
    "            \"center_gene_ids\": self.tokenized_train[\"center_genes\"],\n",
    "            \"input_center_values\": self.tokenized_train[\"center_values\"],\n",
    "            \"niche_gene_ids\": self.tokenized_train[\"niche_genes\"],\n",
    "            \"input_niche_values\": self.tokenized_train[\"niche_values\"],\n",
    "            \"cross_attn_bias\": self.tokenized_train[\"cross_attn_bias\"],\n",
    "            \"center_labels\": self.tokenized_train[\"center_labels\"],\n",
    "        }\n",
    "\n",
    "        self.valid_data_pt = {\n",
    "            \"center_gene_ids\": self.tokenized_valid[\"center_genes\"],\n",
    "            \"input_center_values\": self.tokenized_valid[\"center_values\"],\n",
    "            \"niche_gene_ids\": self.tokenized_valid[\"niche_genes\"],\n",
    "            \"input_niche_values\": self.tokenized_valid[\"niche_values\"],\n",
    "            \"cross_attn_bias\": self.tokenized_valid[\"cross_attn_bias\"],\n",
    "            \"center_labels\": self.tokenized_valid[\"center_labels\"],\n",
    "        }\n",
    "    \n",
    "    def prepare_dataloader(self, batch_size):\n",
    "        train_loader = DataLoader(\n",
    "            dataset=SeqDataset(self.train_data_pt),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=min(len(os.sched_getaffinity(0)), batch_size // 2),\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        valid_loader = DataLoader(\n",
    "            dataset=SeqDataset(self.valid_data_pt),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=min(len(os.sched_getaffinity(0)), batch_size // 2),\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return train_loader, valid_loader\n",
    "\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"center_gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model: nn.Module, loader: DataLoader, scaler, optimizer, scheduler, log_interval, epoch, mode) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    amp = True\n",
    "\n",
    "    model.train()\n",
    "    total_gcl = 0.0\n",
    "    total_error = 0.0\n",
    " \n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        center_gene_ids = batch_data[\"center_gene_ids\"].to(device)\n",
    "        if center_gene_ids.size(0)<8:\n",
    "            continue\n",
    "        input_center_values = batch_data[\"input_center_values\"].to(device)\n",
    "        niche_gene_ids = batch_data[\"niche_gene_ids\"].to(device)\n",
    "        input_niche_values = batch_data[\"input_niche_values\"].to(device)\n",
    "        cross_attn_bias = batch_data[\"cross_attn_bias\"].to(device)\n",
    "        center_labels = batch_data[\"center_labels\"].to(device)\n",
    "\n",
    "        if mode == 'sp':\n",
    "            encoder_src_key_padding_mask = niche_gene_ids.eq(vocab[pad_token])\n",
    "        elif mode == 'sc':\n",
    "            encoder_src_key_padding_mask = torch.ones_like(niche_gene_ids, dtype=torch.bool).to(device)\n",
    "        decoder_src_key_padding_mask = center_gene_ids.eq(vocab[pad_token])\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=amp):\n",
    "            output_dict = model(\n",
    "                    niche_gene_ids,\n",
    "                    input_niche_values,\n",
    "                    encoder_src_key_padding_mask,\n",
    "                    center_gene_ids,\n",
    "                    input_center_values,\n",
    "                    decoder_src_key_padding_mask,\n",
    "                    cross_attn_bias,\n",
    "                    GCL = True,\n",
    "                )\n",
    "    \n",
    "            gcl_output = output_dict[\"gcl_output\"]\n",
    "            batch_logits = gcl_output[torch.logical_or(center_labels==1, center_labels==0)]\n",
    "            batch_labels = center_labels[torch.logical_or(center_labels==1, center_labels==0)]\n",
    "            targets = center_gene_ids[torch.logical_or(center_labels==1, center_labels==0)]\n",
    "            loss_gcl = 0.0\n",
    "            for t in set(targets):\n",
    "                loss_gcl += criterion_cls(batch_logits[targets==t], batch_labels[targets==t])\n",
    "            loss_gcl = loss_gcl/len(set(targets))\n",
    "\n",
    "            error_rate_gcl = 1 - (\n",
    "                    (batch_logits.argmax(1) == batch_labels)\n",
    "                    .sum()\n",
    "                    .item()\n",
    "                ) / batch_labels.size(0)\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss_gcl).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_gcl += loss_gcl.item()\n",
    "        total_error += error_rate_gcl\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            sec_per_batch = (time.time() - start_time) / log_interval\n",
    "            cur_gcl = total_gcl / log_interval\n",
    "            cur_error = total_error / log_interval\n",
    "            logger.info(\n",
    "                f\"| Split {split} | epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.8f} | sec/batch {sec_per_batch:5.1f} | \"\n",
    "                f\"gcl {cur_gcl:5.5f} | \"\n",
    "                f\"acc {1-cur_error:1.5f} | \"\n",
    "            )\n",
    "            total_gcl = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def py_softmax(vector):\n",
    "\te = np.exp(vector)\n",
    "\treturn e / e.sum()\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, mode, curve) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    amp = True\n",
    "    \n",
    "    model.eval()\n",
    "    total_gcl = 0.0\n",
    "    total_error = 0.0\n",
    "    total_num = 0\n",
    "\n",
    "    logits = []\n",
    "    labels = []\n",
    "    \n",
    "    # batch_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(loader):\n",
    "            # batch_num += 1\n",
    "            # if batch_num>100:\n",
    "            #     break\n",
    "            center_gene_ids = batch_data[\"center_gene_ids\"].to(device)\n",
    "            if center_gene_ids.size(0)<8:\n",
    "                continue\n",
    "            input_center_values = batch_data[\"input_center_values\"].to(device)\n",
    "            niche_gene_ids = batch_data[\"niche_gene_ids\"].to(device)\n",
    "            input_niche_values = batch_data[\"input_niche_values\"].to(device)\n",
    "            cross_attn_bias = batch_data[\"cross_attn_bias\"].to(device)\n",
    "            center_labels = batch_data[\"center_labels\"].to(device)\n",
    "\n",
    "            if mode == 'sp':\n",
    "                encoder_src_key_padding_mask = niche_gene_ids.eq(vocab[pad_token])\n",
    "            elif mode == 'sc':\n",
    "                encoder_src_key_padding_mask = torch.ones_like(niche_gene_ids, dtype=torch.bool).to(device)\n",
    "            decoder_src_key_padding_mask = center_gene_ids.eq(vocab[pad_token])\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=amp):\n",
    "                output_dict = model(\n",
    "                        niche_gene_ids,\n",
    "                        input_niche_values,\n",
    "                        encoder_src_key_padding_mask,\n",
    "                        center_gene_ids,\n",
    "                        input_center_values,\n",
    "                        decoder_src_key_padding_mask,\n",
    "                        cross_attn_bias,\n",
    "                        GCL = True,\n",
    "                    )\n",
    "                gcl_output = output_dict[\"gcl_output\"]\n",
    "                batch_logits = gcl_output[torch.logical_or(center_labels==1, center_labels==0)]\n",
    "                batch_labels = center_labels[torch.logical_or(center_labels==1, center_labels==0)]\n",
    "                logits.append(batch_logits.to('cpu'))\n",
    "                labels.append(batch_labels.to('cpu'))\n",
    "\n",
    "            accuracy = (batch_logits.argmax(1) == batch_labels).sum().item()\n",
    "            total_error += batch_labels.size(0) - accuracy\n",
    "            total_num += batch_labels.size(0)\n",
    "            total_gcl += criterion_cls(batch_logits, batch_labels).item()*batch_labels.size(0) \n",
    "\n",
    "    logits = torch.concat(logits).float()\n",
    "    labels = torch.concat(labels)\n",
    "\n",
    "    y_score = [py_softmax(item)[1] for item in logits.numpy()]\n",
    "    y_true = labels.numpy()\n",
    "\n",
    "    if curve == 'roc':\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        auc_value = auc(fpr, tpr)\n",
    "    elif curve == 'prc':\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "        auc_value = auc(recall, precision)\n",
    "\n",
    "    val_err = total_error / total_num\n",
    "    val_loss = total_gcl / total_num\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"valid accuracy: {1-val_err:1.4f} | \"\n",
    "        f\"valid auc: {auc_value:1.4f} | \"\n",
    "        f\"valid loss: {val_loss:1.4f} | \"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "    \n",
    "    if curve == 'roc':\n",
    "        return auc_value, fpr, tpr\n",
    "    elif curve == 'prc':\n",
    "        return auc_value, recall, precision\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_loader, valid_loader, epochs, mean_curve_x, mode, curve):\n",
    "    lr = 1e-4\n",
    "    amp = True\n",
    "    schedule_ratio = 0.9\n",
    "    schedule_interval = 1\n",
    "    log_interval = 10\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=lr, eps=1e-4 if amp else 1e-8\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, schedule_interval, gamma=schedule_ratio\n",
    "    )\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "    best_val_auc = 0\n",
    "    best_curve_x = 0\n",
    "    best_curve_y = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train(model, train_loader, scaler, optimizer, scheduler, log_interval, epoch, mode)\n",
    "\n",
    "        val_auc, curve_x, curve_y = evaluate(model, valid_loader, mode, curve)\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_curve_x = curve_x\n",
    "            best_curve_y = curve_y\n",
    "            logger.info(f\"Best model with auc {best_val_auc:1.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "    \n",
    "    if curve == 'roc':\n",
    "        interp_curve_y = np.interp(mean_curve_x, best_curve_x, best_curve_y)\n",
    "        interp_curve_y[0] = 0.0\n",
    "    elif curve == 'prc':\n",
    "        interp_curve_y = np.interp(mean_curve_x, best_curve_x[::-1], best_curve_y[::-1])\n",
    "        interp_curve_y[0] = 1.0\n",
    "    \n",
    "    return interp_curve_y, best_curve_x, best_curve_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scfoundation import load\n",
    "\n",
    "def initialize_model(model_file):\n",
    "    pretrainmodel, pretrainconfig = load.load_model_frommmf('scfoundation/models/models.ckpt')\n",
    "\n",
    "    model = TransformerModel(\n",
    "        embsize,\n",
    "        nhead,\n",
    "        d_hid,\n",
    "        nlayers,\n",
    "        do_gcl = True,\n",
    "        nlayers_gcl = 3,\n",
    "        n_gcl =2,\n",
    "        dropout = dropout,\n",
    "        cell_emb_style = cell_emb_style,\n",
    "        scfoundation_token_emb1 = copy.deepcopy(pretrainmodel.token_emb),\n",
    "        scfoundation_token_emb2 = copy.deepcopy(pretrainmodel.token_emb),\n",
    "        scfoundation_pos_emb1 = copy.deepcopy(pretrainmodel.pos_emb),\n",
    "        scfoundation_pos_emb2 = copy.deepcopy(pretrainmodel.pos_emb),\n",
    "    )\n",
    "\n",
    "    pt_model = torch.load(model_file, map_location='cpu')\n",
    "\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = pt_model.state_dict()\n",
    "    pretrained_dict = {\n",
    "                k: v\n",
    "                for k, v in pretrained_dict.items()\n",
    "                if 'cls_decoder' not in k and 'gcl_decoder' not in k\n",
    "                # if k in model_dict and v.shape == model_dict[k].shape\n",
    "    }\n",
    "    # for k, v in pretrained_dict.items():\n",
    "    #     logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "    for name, para in model.named_parameters():\n",
    "        para.requires_grad = False\n",
    "    for name, para in model.transformer_decoder.layers[4:6].named_parameters():\n",
    "        para.requires_grad = True\n",
    "    for name, para in model.gcl_decoder.named_parameters():\n",
    "        para.requires_grad = True\n",
    "    post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "    logger.info(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "    logger.info(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "\n",
    "    return model        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embsize = 768\n",
    "d_hid = 3072\n",
    "nhead = 12\n",
    "nlayers = 6\n",
    "dropout = 0.1\n",
    "cell_emb_style = 'max-pool'\n",
    "GCL = True\n",
    "mode = 'sp'\n",
    "curve = 'prc'\n",
    "\n",
    "pad_token = \"<pad>\"\n",
    "pad_value = 103\n",
    "tokenizer_dir = '../stformer/tokenizer/'\n",
    "vocab_file = tokenizer_dir + \"scfoundation_gene_vocab.json\"\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "vocab.append_token(pad_token)\n",
    "vocab.set_default_index(vocab[pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = '../pretraining/models/model_4.1M.ckpt'\n",
    "label_path = 'gene_lists/TGF-beta_signaling_pathway_KEGG.csv'\n",
    "pathway_ligands = ['BMP2','BMP4','BMP5','BMP6','BMP7','BMP8A','BMP8B','GDF5','GDF6','GDF7','INHBA','INHBB','INHBC','INHBE','TGFB1','TGFB2','TGFB3','NODAL']\n",
    "\n",
    "dataset = 'human_myocardial_infarction'\n",
    "slide = 'ACH005'\n",
    "adata = sc.read_h5ad(f'../datasets/{slide}_niche.h5ad')\n",
    "\n",
    "# max_niche_cell_num = 20 # [1,5,10,15,20,25]\n",
    "# adata.obsm['niche_celltypes'] = adata.obsm[f'niche_celltypes_niche{max_niche_cell_num}']\n",
    "# adata.obsm['niche_composition'] = adata.obsm[f'niche_composition_niche{max_niche_cell_num}']\n",
    "# adata.obsm['niche_ligands_expression'] = adata.obsm[f'niche_ligands_expression_niche{max_niche_cell_num}']\n",
    "\n",
    "# selected_celltype = 'Fibroblast'\n",
    "# adata = adata[adata.obs['cell_type']==list(adata.uns['cell_types_list']).index(selected_celltype)+1]\n",
    "np.random.seed(0)\n",
    "shuffled_indices = np.random.permutation(adata.n_obs)\n",
    "adata = adata[shuffled_indices]\n",
    "# adata = adata[:2000]\n",
    "\n",
    "tokenizer = Tokenizer(tokenizer_dir, adata, label_path, vocab, pad_value, pad_token, pathway_ligands)\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 30\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "mean_curve_x = np.linspace(0, 1, 100)\n",
    "all_curve_y = []\n",
    "all_auc = []\n",
    "all_wt = []\n",
    "\n",
    "split = 0\n",
    "for train_index, valid_index in skf.split(tokenizer.gene_targets, tokenizer.gene_labels):\n",
    "    split += 1\n",
    "    logger.info(f\"Cross-validate on dataset {dataset} slide {slide} - split {split}\")\n",
    "    tokenizer.tokenize_data(train_index, valid_index)\n",
    "    tokenizer.prepare_data()\n",
    "    train_loader, valid_loader = tokenizer.prepare_dataloader(batch_size)\n",
    "    \n",
    "    model = initialize_model(model_file)\n",
    "    model = nn.DataParallel(model, device_ids = [0, 1, 2])\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model.to(device)\n",
    "\n",
    "    interp_curve_y, best_curve_x, best_curve_y = train_and_evaluate(model, train_loader, valid_loader, epochs, mean_curve_x, mode, curve)\n",
    "    all_curve_y.append(interp_curve_y)\n",
    "    all_auc.append(auc(best_curve_x, best_curve_y))\n",
    "    all_wt.append(len(best_curve_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def get_cross_valid_metrics(all_curve_y, all_auc, all_wt, curve):\n",
    "    wts = [count/sum(all_wt) for count in all_wt]\n",
    "    print(wts)\n",
    "    all_weighted_curve_y = [a*b for a,b in zip(all_curve_y, wts)]\n",
    "    mean_curve_y = np.sum(all_weighted_curve_y, axis=0)\n",
    "    if curve == 'roc':\n",
    "        mean_curve_y[-1] = 1.0\n",
    "    all_weighted_auc = [a*b for a,b in zip(all_auc, wts)]\n",
    "    auc_mean = np.sum(all_weighted_auc)\n",
    "    auc_sd = math.sqrt(np.average((all_auc-auc_mean)**2, weights=wts))\n",
    "    return mean_curve_y, auc_mean, auc_sd, wts\n",
    " \n",
    "mean_curve_y, auc_mean, auc_sd, wts = get_cross_valid_metrics(all_curve_y, all_auc, all_wt, curve)\n",
    "\n",
    "print(f\"Mean AUC: {auc_mean} +/- {auc_sd}\")\n",
    "cv_results = {'auc_mean':auc_mean, 'auc_sd':auc_sd, 'mean_curve_x':mean_curve_x, 'mean_curve_y':mean_curve_y, 'all_auc':all_auc, 'wts':wts, 'all_curve_y':all_curve_y}\n",
    "pickle.dump(cv_results, open(f'figures/gene_classification/stformer_gcl_{slide}_nfkb_{curve}.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ROC(bundled_data, title, curve):\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    for auc_mean, auc_sd, mean_curve_x, mean_curve_y, sample, color in bundled_data:\n",
    "        plt.plot(mean_curve_x, mean_curve_y, color=color,\n",
    "                 lw=lw, label=\"{0} (AUC {1:0.3f} $\\pm$ {2:0.3f})\".format(sample, auc_mean, auc_sd))\n",
    "    if curve == 'roc':\n",
    "        plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "    elif curve == 'prc':\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "bundled_data = [(auc_mean, auc_sd, mean_curve_x, mean_curve_y, \"stFormer\", \"red\")]\n",
    "\n",
    "plot_ROC(bundled_data, 'Gene classification', curve)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stformer",
   "language": "python",
   "name": "stformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
